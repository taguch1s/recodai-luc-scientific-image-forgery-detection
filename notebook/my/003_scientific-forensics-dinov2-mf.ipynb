{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":113558,"databundleVersionId":14878066,"sourceType":"competition"},{"sourceId":270092713,"sourceType":"kernelVersion"},{"sourceId":4534,"sourceType":"modelInstanceVersion","modelInstanceId":3326,"modelId":986}],"dockerImageVersionId":31236,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## metric\n","metadata":{}},{"cell_type":"code","source":"import json\n\nimport numba\nimport numpy as np\nfrom numba import types\nimport numpy.typing as npt\nimport pandas as pd\nimport scipy.optimize\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\n@numba.jit(nopython=True)\ndef _rle_encode_jit(x: npt.NDArray, fg_val: int = 1) -> list[int]|str:\n    \"\"\"Numba-jitted RLE encoder.\"\"\"\n    dots = np.where(x.T.flatten() == fg_val)[0]\n    if len(dots) == 0:\n        return \"authentic\"\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if b > prev + 1:\n            run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\n\ndef rle_encode(masks: list[npt.NDArray], fg_val: int = 1) -> str:\n    \"\"\"\n    Adapted from contrails RLE https://www.kaggle.com/code/inversion/contrails-rle-submission\n    Args:\n        masks: list of numpy array of shape (height, width), 1 - mask, 0 - background\n    Returns: run length encodings as a string, with each RLE JSON-encoded and separated by a semicolon.\n    \"\"\"\n    return ';'.join([json.dumps(_rle_encode_jit(x, fg_val)) for x in masks])\n\n\n@numba.njit\ndef _rle_decode_jit(mask_rle: npt.NDArray, height: int, width: int) -> npt.NDArray:\n    \"\"\"\n    s: numpy array of run-length encoding pairs (start, length)\n    shape: (height, width) of array to return\n    Returns numpy array, 1 - mask, 0 - background\n    \"\"\"\n    if len(mask_rle) % 2 != 0:\n        # Numba requires raising a standard exception.\n        raise ValueError('One or more rows has an odd number of values.')\n\n    starts, lengths = mask_rle[0::2], mask_rle[1::2]\n    starts -= 1\n    ends = starts + lengths\n    for i in range(len(starts) - 1):\n        if ends[i] > starts[i + 1]:\n            raise ValueError('Pixels must not be overlapping.')\n    img = np.zeros(height * width, dtype=np.bool_)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img\n\n\ndef rle_decode(mask_rle: str, shape: tuple[int, int]) -> npt.NDArray:\n    \"\"\"\n    mask_rle: run-length as string formatted (start length)\n              empty predictions need to be encoded with '-'\n    shape: (height, width) of array to return\n    Returns numpy array, 1 - mask, 0 - background\n    \"\"\"\n\n    mask_rle = json.loads(mask_rle)\n    mask_rle = np.asarray(mask_rle, dtype=np.int32)\n    starts = mask_rle[0::2]\n    if sorted(starts) != list(starts):\n        raise ParticipantVisibleError('Submitted values must be in ascending order.')\n    try:\n        return _rle_decode_jit(mask_rle, shape[0], shape[1]).reshape(shape, order='F')\n    except ValueError as e:\n        raise ParticipantVisibleError(str(e)) from e\n\n\ndef calculate_f1_score(pred_mask: npt.NDArray, gt_mask: npt.NDArray):\n    pred_flat = pred_mask.flatten()\n    gt_flat = gt_mask.flatten()\n\n    tp = np.sum((pred_flat == 1) & (gt_flat == 1))\n    fp = np.sum((pred_flat == 1) & (gt_flat == 0))\n    fn = np.sum((pred_flat == 0) & (gt_flat == 1))\n\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n\n    if (precision + recall) > 0:\n        return 2 * (precision * recall) / (precision + recall)\n    else:\n        return 0\n\n\ndef calculate_f1_matrix(pred_masks: list[npt.NDArray], gt_masks: list[npt.NDArray]):\n    \"\"\"\n    Parameters:\n    pred_masks (np.ndarray):\n            First dimension is the number of predicted instances.\n            Each instance is a binary mask of shape (height, width).\n    gt_masks (np.ndarray):\n            First dimension is the number of ground truth instances.\n            Each instance is a binary mask of shape (height, width).\n    \"\"\"\n\n    num_instances_pred = len(pred_masks)\n    num_instances_gt = len(gt_masks)\n    f1_matrix = np.zeros((num_instances_pred, num_instances_gt))\n\n    # Calculate F1 scores for each pair of predicted and ground truth masks\n    for i in range(num_instances_pred):\n        for j in range(num_instances_gt):\n            pred_flat = pred_masks[i].flatten()\n            gt_flat = gt_masks[j].flatten()\n            f1_matrix[i, j] = calculate_f1_score(pred_mask=pred_flat, gt_mask=gt_flat)\n\n    if f1_matrix.shape[0] < len(gt_masks):\n        # Add a row of zeros to the matrix if the number of predicted instances is less than ground truth instances\n        f1_matrix = np.vstack((f1_matrix, np.zeros((len(gt_masks) - len(f1_matrix), num_instances_gt))))\n\n    return f1_matrix\n\n\ndef oF1_score(pred_masks: list[npt.NDArray], gt_masks: list[npt.NDArray]):\n    \"\"\"\n    Calculate the optimal F1 score for a set of predicted masks against\n    ground truth masks which considers the optimal F1 score matching.\n    This function uses the Hungarian algorithm to find the optimal assignment\n    of predicted masks to ground truth masks based on the F1 score matrix.\n    If the number of predicted masks is less than the number of ground truth masks,\n    it will add a row of zeros to the F1 score matrix to ensure that the dimensions match.\n\n    Parameters:\n    pred_masks (list of np.ndarray): List of predicted binary masks.\n    gt_masks (np.ndarray): Array of ground truth binary masks.\n    Returns:\n    float: Optimal F1 score.\n    \"\"\"\n    f1_matrix = calculate_f1_matrix(pred_masks, gt_masks)\n\n    # Find the best matching between predicted and ground truth masks\n    row_ind, col_ind = scipy.optimize.linear_sum_assignment(-f1_matrix)\n    # The linear_sum_assignment discards excess predictions so we need a separate penalty.\n    excess_predictions_penalty = len(gt_masks) / max(len(pred_masks), len(gt_masks))\n    return np.mean(f1_matrix[row_ind, col_ind]) * excess_predictions_penalty\n\n\ndef evaluate_single_image(label_rles: str, prediction_rles: str, shape_str: str) -> float:\n    shape = json.loads(shape_str)\n    label_rles = [rle_decode(x, shape=shape) for x in label_rles.split(';')]\n    prediction_rles = [rle_decode(x, shape=shape) for x in prediction_rles.split(';')]\n    return oF1_score(prediction_rles, label_rles)\n\n\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Args:\n        solution (pd.DataFrame): The ground truth DataFrame.\n        submission (pd.DataFrame): The submission DataFrame.\n        row_id_column_name (str): The name of the column containing row IDs.\n    Returns:\n        float\n\n    Examples\n    --------\n    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['authentic', 'authentic', 'authentic'], 'shape': ['authentic', 'authentic', 'authentic']})\n    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['authentic', 'authentic', 'authentic']})\n    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n    1.0\n\n    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['authentic', 'authentic', 'authentic'], 'shape': ['authentic', 'authentic', 'authentic']})\n    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]']})\n    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n    0.0\n\n    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]'], 'shape': ['[720, 960]', '[720, 960]', '[720, 960]']})\n    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]']})\n    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n    1.0\n\n    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 103]', '[101, 102]', '[101, 102]'], 'shape': ['[720, 960]', '[720, 960]', '[720, 960]']})\n    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]']})\n    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n    0.9983739837398374\n\n    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102];[300, 100]', '[101, 102]', '[101, 102]'], 'shape': ['[720, 960]', '[720, 960]', '[720, 960]']})\n    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]']})\n    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n    0.8333333333333334\n    \"\"\"\n    df = solution\n    df = df.rename(columns={'annotation': 'label'})\n\n    df['prediction'] = submission['annotation']\n    # Check for correct 'authentic' label\n    authentic_indices = (df['label'] == 'authentic') | (df['prediction'] == 'authentic')\n    df['image_score'] = ((df['label'] == df['prediction']) & authentic_indices).astype(float)\n\n    df.loc[~authentic_indices, 'image_score'] = df.loc[~authentic_indices].apply(\n        lambda row: evaluate_single_image(row['label'], row['prediction'], row['shape']), axis=1\n    )\n    return float(np.mean(df['image_score']))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:37:45.505459Z","iopub.execute_input":"2026-01-10T07:37:45.506194Z","iopub.status.idle":"2026-01-10T07:37:46.296122Z","shell.execute_reply.started":"2026-01-10T07:37:45.506162Z","shell.execute_reply":"2026-01-10T07:37:46.295460Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# --- Transformer-based DinoMFDecoder (Mask2Former-style, single-scale minimal) ---\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Tuple\n\nclass _MaskDecoderLayer(nn.Module):\n    \"\"\"\n    One decoder stage composed of CA (cross-attention), SA (self-attention), and FFN.\n    - CA: queries attend to pixel tokens (image features)\n    - SA: queries attend to themselves to resolve mutual relations\n    - FFN: position-wise feed-forward for non-linearity\n    Order here follows the diagram: CA -> SA -> FFN.\n    \"\"\"\n    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 1024, dropout: float = 0.1):\n        super().__init__()\n        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n        self.self_attn  = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n        self.linear1    = nn.Linear(d_model, dim_feedforward)\n        self.linear2    = nn.Linear(dim_feedforward, d_model)\n        self.dropout    = nn.Dropout(dropout)\n        self.dropout1   = nn.Dropout(dropout)\n        self.dropout2   = nn.Dropout(dropout)\n        self.norm1      = nn.LayerNorm(d_model)\n        self.norm2      = nn.LayerNorm(d_model)\n        self.norm3      = nn.LayerNorm(d_model)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, q: torch.Tensor, kv: torch.Tensor, kv_pos: torch.Tensor = None):\n        # Cross-Attention (queries <- pixels)\n        k = v = kv if kv_pos is None else kv + kv_pos\n        q2, _ = self.cross_attn(query=q, key=k, value=v, need_weights=False)\n        q = q + self.dropout1(q2)\n        q = self.norm1(q)\n        # Self-Attention (queries <- queries)\n        q2, _ = self.self_attn(query=q, key=q, value=q, need_weights=False)\n        q = q + self.dropout2(q2)\n        q = self.norm2(q)\n        # FFN\n        q2 = self.linear2(self.dropout(self.activation(self.linear1(q))))\n        q = self.norm3(q + q2)\n        return q\n\nclass DinoMFDecoder(nn.Module):\n    \"\"\"\n    Mask2Former-like minimal decoder:\n    - Projects DINOv2 fmap (B,C,h,w) -> (B,D,h,w) as pixel embeddings\n    - Uses learnable queries (Q,D)\n    - L stages of [CA -> SA -> FFN]\n    - Produces Q masks via dot-product between query mask-embeds and pixel embeddings\n    Returns a single-channel mask by max-pooling across queries (compatible with existing pipeline).\n    \"\"\"\n    def __init__(\n        self,\n        in_ch: int = 768,\n        out_ch: int = 1,\n        d_model: int = 256,\n        nhead: int = 8,\n        num_queries: int = 64,\n        num_layers: int = 2,\n        dim_feedforward: int = 1024,\n        dropout: float = 0.1,\n    ):\n        super().__init__()\n        self.d_model = d_model\n        self.pixel_proj = nn.Conv2d(in_ch, d_model, kernel_size=1)\n        self.query_embed = nn.Embedding(num_queries, d_model)\n        self.layers = nn.ModuleList([\n            _MaskDecoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_layers)\n        ])\n        self.mask_embed = nn.Linear(d_model, d_model)\n        self.class_embed = nn.Linear(d_model, 2)  # forged / void (optional)\n        # simple 2D sine-cos positional encoding for pixels\n        self.register_buffer(\"pe_cache\", None, persistent=False)\n\n    def _pos2d(self, H: int, W: int, device) -> torch.Tensor:\n        # cached for a given HxW\n        if self.pe_cache is not None and self.pe_cache.shape[1] == H*W:\n            return self.pe_cache\n        y, x = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing=\"ij\")\n        omega = torch.arange(self.d_model // 4, device=device, dtype=torch.float32) / (self.d_model // 4)\n        omega = 1.0 / (10000 ** omega)\n        out_y = torch.cat([torch.sin(y.reshape(-1,1) * omega), torch.cos(y.reshape(-1,1) * omega)], dim=1)\n        out_x = torch.cat([torch.sin(x.reshape(-1,1) * omega), torch.cos(x.reshape(-1,1) * omega)], dim=1)\n        pe = torch.cat([out_y, out_x], dim=1)\n        if pe.shape[1] < self.d_model:\n            pe = F.pad(pe, (0, self.d_model - pe.shape[1]))\n        else:\n            pe = pe[:, :self.d_model]\n        pe = pe.unsqueeze(0)  # (1, HW, D)\n        self.pe_cache = pe\n        return pe\n\n    def forward(self, f: torch.Tensor, target_size: Tuple[int, int]):\n        # f: (B, C, h, w)\n        B, C, h, w = f.shape\n        device = f.device\n        pix = self.pixel_proj(f)                 # (B, D, h, w)\n        pix_flat = pix.flatten(2).permute(0, 2, 1)  # (B, HW, D)\n        pos = self._pos2d(h, w, device).expand(B, -1, -1)  # (B, HW, D)\n\n        # prepare queries\n        q = self.query_embed.weight.unsqueeze(0).expand(B, -1, -1)  # (B, Q, D)\n        for layer in self.layers:\n            q = layer(q, kv=pix_flat, kv_pos=pos)\n\n        # class logits (optional, not used downstream yet)\n        cls_logits = self.class_embed(q)  # (B, Q, 2)\n\n        # mask logits via dot product between query mask-embed and pixel embeddings\n        mask_tokens = self.mask_embed(q)              # (B, Q, D)\n        pix_grid = pix                               # (B, D, h, w)\n        mask_logits = torch.einsum('bqd,bdhw->bqhw', mask_tokens, pix_grid)  # (B, Q, h, w)\n\n        # collapse queries by max (keep old interface: out_ch=1)\n        logits_1 = mask_logits.max(dim=1, keepdim=True).values  # (B,1,h,w)\n        logits_1 = F.interpolate(logits_1, size=target_size, mode='bilinear', align_corners=False)\n        return logits_1\n\n# Usage note:\n# - To switch: model_seg.seg_head = DinoMFDecoder(768, 1, d_model=256, nhead=8, num_queries=64, num_layers=2)\n# - This keeps the same forward_seg signature and downstream pipeline.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:37:46.297384Z","iopub.execute_input":"2026-01-10T07:37:46.297752Z","iopub.status.idle":"2026-01-10T07:37:48.068650Z","shell.execute_reply.started":"2026-01-10T07:37:46.297728Z","shell.execute_reply":"2026-01-10T07:37:48.068071Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:37:48.069556Z","iopub.execute_input":"2026-01-10T07:37:48.070083Z","iopub.status.idle":"2026-01-10T07:37:48.073468Z","shell.execute_reply.started":"2026-01-10T07:37:48.070059Z","shell.execute_reply":"2026-01-10T07:37:48.072944Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os, cv2, json, math, random, torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn, torch.nn.functional as F, torch.optim as optim\nfrom transformers import AutoImageProcessor, AutoModel\nfrom metric import *\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # This forces CUDA to use deterministic algorithms (slower but consistent)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- Environment detection & dynamic paths (Kaggle vs Colab) ---\nIS_KAGGLE = os.path.exists('/kaggle/input/recodai-luc-scientific-image-forgery-detection')\nIS_COLAB = 'google.colab' in globals() or 'COLAB_GPU' in os.environ\n\nBASE_DIR  = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\" if IS_KAGGLE else \"/content/input/\"\n\nAUTH_DIR  = f\"{BASE_DIR}/train_images/authentic\"\nFORG_DIR  = f\"{BASE_DIR}/train_images/forged\"\nMASK_DIR  = f\"{BASE_DIR}/train_masks\"\nTEST_DIR  = f\"{BASE_DIR}/test_images\"\n\n# DINO model reference: Kaggle dataset path or Hugging Face ID for Colab\nDINO_PATH = \"/kaggle/input/dinov2/pytorch/base/1\" if IS_KAGGLE else os.environ.get(\"DINO_ID\", \"facebook/dinov2-base\")\n\n# On Kaggle, prefer local files only; on Colab, allow remote download\nUSE_LOCAL_ONLY = True if IS_KAGGLE else False\n\nIMG_SIZE = 512\nBATCH_SIZE = 2\n# MODEL_LOC examples for Kaggle (optional). In Colab, set to None or a local path under /content.\n# MODEL_LOC = '/kaggle/input/cnndinov2-pbd/CNNDINOv2-U54/CNNDINOv2-U54/model_seg_final.pt'  # 0.310\n# MODEL_LOC = '/kaggle/input/cnndinov2-pbd/CNNDINOv2-U52/CNNDINOv2-U52/model_seg_final.pt'  # 0.321\nMODEL_LOC = '/kaggle/input/dinov2-mfdecorder/pytorch/v1/1/model_seg_final.pt' if IS_KAGGLE else None\nEPOCHS_SEG = 4\nLR_SEG = 1e-5\nWEIGHT_DECAY = 2e-5\nVAL_SAMPLES = 10 # validate on a small forged subset per epoch\n\n# INFERENCE UTILS\nAREA_THR = 200\nMEAN_THR = 0.22\nUSE_TTA = True\nGRID_SEARCH = False\n\nclass ForgerySegDataset(Dataset):\n    def __init__(self, auth_paths, forg_paths, mask_dir, img_size=IMG_SIZE):\n        self.samples = []\n        for p in forg_paths:\n            m = os.path.join(mask_dir, Path(p).stem + \".npy\")\n            if os.path.exists(m):\n                self.samples.append((p, m))\n        for p in auth_paths:\n            self.samples.append((p, None))\n        self.img_size = img_size\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, idx):\n        img_path, mask_path = self.samples[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        w, h = img.size\n        if mask_path is None:\n            mask = np.zeros((h, w), np.uint8)\n        else:\n            m = np.load(mask_path)\n            if m.ndim == 3: m = np.max(m, axis=0)\n            mask = (m > 0).astype(np.uint8)\n        img_r = img.resize((IMG_SIZE, IMG_SIZE))\n        mask_r = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n        img_t = torch.from_numpy(np.array(img_r, np.float32)/255.).permute(2,0,1)\n        mask_t = torch.from_numpy(mask_r[None, ...].astype(np.float32))\n        return img_t, mask_t\n\n\n#  MODEL (DINOv2 + Decoder)\n\nprocessor = AutoImageProcessor.from_pretrained(DINO_PATH, local_files_only=USE_LOCAL_ONLY, use_fast=False)\nencoder = AutoModel.from_pretrained(DINO_PATH, local_files_only=USE_LOCAL_ONLY).eval().to(device)\n\nclass DinoTinyDecoder(nn.Module):\n    def __init__(self, in_ch=768, out_ch=1):\n        super().__init__()\n        # Block 1: 768 -> 384\n        self.block1 = nn.Sequential(\n            nn.Conv2d(in_ch, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1)\n        )\n        # Block 2: 384 -> 192\n        self.block2 = nn.Sequential(\n            nn.Conv2d(384, 192, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1)\n        )\n        # Block 3: 192 -> 96\n        self.block3 = nn.Sequential(\n            nn.Conv2d(192, 96, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n        # Final Output: 96 -> 1\n        self.conv_out = nn.Conv2d(96, out_ch, kernel_size=1)\n\n    def forward(self, f, target_size):\n        # f: [B, 768, 37, 37]\n\n        # Step 1: Up to ~74x74\n        x = F.interpolate(self.block1(f), size=(74, 74), mode='bilinear', align_corners=False)\n\n        # Step 2: Up to ~148x148\n        x = F.interpolate(self.block2(x), size=(148, 148), mode='bilinear', align_corners=False)\n\n        # Step 3: Up to ~296x296\n        x = F.interpolate(self.block3(x), size=(296, 296), mode='bilinear', align_corners=False)\n\n        # Step 4: Final jump to 518x518\n        x = self.conv_out(x)\n        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n\n        return x\n\nclass DinoSegmenter(nn.Module):\n    def __init__(self, encoder, processor):\n        super().__init__()\n        self.encoder, self.processor = encoder, processor\n        for p in self.encoder.parameters(): p.requires_grad = False\n        # self.seg_head = DinoMFDecoder(768,1)\n        self.seg_head = DinoTinyDecoder(768,1)\n\n    def forward_features(self,x):\n        imgs = (x*255).clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()\n        inputs = self.processor(images=list(imgs), return_tensors=\"pt\").to(x.device)\n        # with torch.no_grad():\n        #     feats = self.encoder(**inputs).last_hidden_state\n        feats = self.encoder(**inputs).last_hidden_state\n        B,N,C = feats.shape\n        fmap = feats[:,1:,:].permute(0,2,1)\n        s = int(math.sqrt(N-1))\n        fmap = fmap.reshape(B,C,s,s)\n        return fmap\n    def forward_seg(self,x):\n        fmap = self.forward_features(x)\n        return self.seg_head(fmap,(IMG_SIZE,IMG_SIZE))\n\n# --- 3.1 Utilities: init bias, losses, small eval ---\n\ndef init_last_conv_bias_negative(module: nn.Module, bias_value: float = -4.0):\n    \"\"\"Initialize the final 1-channel Conv2d bias to a negative value to avoid early saturation.\"\"\"\n    for m in module.modules():\n        if isinstance(m, nn.Conv2d) and m.out_channels == 1:\n            if m.bias is not None:\n                nn.init.constant_(m.bias, bias_value)\n\nclass SoftDiceLoss(nn.Module):\n    def __init__(self, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        probs = torch.sigmoid(logits)\n        targets = targets.float()\n        inter = (probs * targets).sum(dim=(2,3))\n        denom = probs.sum(dim=(2,3)) + targets.sum(dim=(2,3)) + self.eps\n        dice = (2 * inter + self.eps) / denom\n        return 1 - dice.mean()\n\n@torch.no_grad()\ndef evaluate_mean_f1(model_seg, val_list, mask_dir, max_items=VAL_SAMPLES):\n    model_seg.eval()\n    items = val_list[:max_items]\n    f1s = []\n    for p in items:\n        pil = Image.open(p).convert(\"RGB\")\n        x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n        logits = model_seg.forward_seg(x)\n        prob = torch.sigmoid(logits)[0,0].cpu().numpy()\n        # simple mask (same as pipeline)\n        gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n        gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n        grad_mag = np.sqrt(gx**2 + gy**2)\n        grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n        enhanced = 0.65 * prob + 0.35 * grad_norm\n        enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n        thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n        mask = (enhanced > thr).astype(np.uint8)\n        m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n        if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n        m_gt = (m_gt > 0).astype(np.uint8)\n        # align to original size\n        mask = cv2.resize(mask, (m_gt.shape[1], m_gt.shape[0]), interpolation=cv2.INTER_NEAREST)\n        f1s.append(f1_score(m_gt.flatten(), mask.flatten(), zero_division=0))\n    return float(np.mean(f1s)) if f1s else 0.0\n\n\n# --- 4. DATA SPLIT AND TRAINING ---\nauth_imgs = sorted([str(Path(AUTH_DIR)/f) for f in os.listdir(AUTH_DIR)])\nforg_imgs = sorted([str(Path(FORG_DIR)/f) for f in os.listdir(FORG_DIR)])\ntrain_auth, val_auth = train_test_split(auth_imgs, test_size=0.2, random_state=42)\ntrain_forg, val_forg = train_test_split(forg_imgs, test_size=0.2, random_state=42)\n\ntrain_loader = DataLoader(ForgerySegDataset(train_auth, train_forg, MASK_DIR),\n                          batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(ForgerySegDataset(val_auth, val_forg, MASK_DIR),\n                        batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\nmodel_seg = DinoSegmenter(encoder, processor).to(device)\n\ninit_last_conv_bias_negative(model_seg.seg_head, bias_value=-4.0)\n\n# Train only decoder\nopt_seg = optim.AdamW(model_seg.seg_head.parameters(), lr=LR_SEG, weight_decay=WEIGHT_DECAY)\ncrit_bce = nn.BCEWithLogitsLoss()\ncrit_dice = SoftDiceLoss()\n\ndef seg_loss(logits, targets, alpha=0.5):\n    return alpha * crit_bce(logits, targets) + (1 - alpha) * crit_dice(logits, targets)\n\nscaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n\ndef train_seg(epoch):\n    best_f1 = -1.0\n    best_path = \"model_seg_best.pt\"\n\n    for e in range(epoch):\n        model_seg.train()\n        model_seg.encoder.eval()  # keep encoder frozen/eval\n        total_loss = 0.0\n\n        for x, m in tqdm(train_loader, desc=f\"[Segmentation] Epoch {e+1}/{epoch}\"):\n            x, m = x.to(device), m.to(device)\n            opt_seg.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n                logits = model_seg.forward_seg(x)\n                loss = seg_loss(logits, m)\n            scaler.scale(loss).backward()\n            scaler.step(opt_seg)\n            scaler.update()\n            total_loss += loss.item()\n\n        avg_loss = total_loss / max(1, len(train_loader))\n\n        # quick validation on forged subset\n        val_f1 = evaluate_mean_f1(model_seg, val_forg, MASK_DIR, max_items=VAL_SAMPLES)\n        print(f\"  ‚Üí avg_loss={avg_loss:.4f} | val_f1@{VAL_SAMPLES}={val_f1:.4f}\")\n\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            torch.save(model_seg.state_dict(), best_path)\n            print(f\"  ‚úÖ New best saved to {best_path}\")\n\n    # Save final model state\n    torch.save(model_seg.state_dict(), \"model_seg_final.pt\")\n    print(\"Model saved as model_seg_final.pt\")\n\n\n# Load pretrained weights if MODEL_LOC is specified\nif MODEL_LOC is not None and os.path.exists(MODEL_LOC):\n    model_seg.load_state_dict(torch.load(MODEL_LOC, map_location=device))\n    print(f\"‚úÖ Loaded pretrained model from: {MODEL_LOC}\")\n    model_seg.eval()  # Set model to evaluation mode\nelse:\n    print(\"model not found, training start!\")\n    train_seg(EPOCHS_SEG)\n\n@torch.no_grad()\ndef segment_prob_map(pil):\n    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n    prob = torch.sigmoid(model_seg.forward_seg(x))[0,0].cpu().numpy()\n    return prob\n\n@torch.no_grad()\ndef segment_prob_map_with_tta(pil):\n    # 1. Preprocessing: Resize, Normalize, and move to Device\n    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n\n    predictions = []\n\n    # 2. Original Prediction\n    pred_orig = torch.sigmoid(model_seg.forward_seg(x))\n    predictions.append(pred_orig)\n\n    # 3. Horizontal Flip TTA (dim 3)\n    # Flip input -> Predict -> Flip output back\n    pred_h = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[3])))\n    predictions.append(torch.flip(pred_h, dims=[3]))\n\n    # 4. Vertical Flip TTA (dim 2)\n    # Flip input -> Predict -> Flip output back\n    pred_v = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[2])))\n    predictions.append(torch.flip(pred_v, dims=[2]))\n\n    # 5. Average the predictions and format as numpy\n    # We stack the 3 predictions and take the mean across the stack dimension (0)\n    prob = torch.stack(predictions).mean(0)[0, 0].cpu().numpy()\n\n    return prob\n\ndef enhanced_adaptive_mask(prob, alpha_grad=0.45):\n    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n    grad_mag = np.sqrt(gx**2 + gy**2)\n    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n    mask = (enhanced > thr).astype(np.uint8)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n    return mask, thr\n\ndef finalize_mask(prob, orig_size):\n    mask, thr = enhanced_adaptive_mask(prob)\n    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n    return mask, thr\n\ndef pipeline_final(pil):\n    if USE_TTA:\n        prob = segment_prob_map_with_tta(pil)\n    else:\n        prob = segment_prob_map(pil)\n    mask, thr = finalize_mask(prob, pil.size)\n    area = int(mask.sum())\n    mean_inside = float(prob[cv2.resize(mask,(IMG_SIZE,IMG_SIZE),interpolation=cv2.INTER_NEAREST)==1].mean()) if area>0 else 0.0\n    if area < AREA_THR or mean_inside < MEAN_THR:\n        return \"authentic\", None, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n    return \"forged\", mask, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n\nimport itertools\n\ndef grid_search_area_mean(forg_paths, auth_paths, mask_dir):\n    mean_range = [round(x, 2) for x in np.arange(0.20, 0.291, 0.01)]\n    area_range = [200]\n    # 1. Use ALL images from both paths to maximize robustness\n    val_set = [(p, \"forged\") for p in forg_paths] + [(p, \"authentic\") for p in auth_paths]\n\n    print(f\"üöÄ Step 1: Caching probability maps for ALL {len(val_set)} images...\")\n    cache = []\n    for p, label in tqdm(val_set):\n        pil = Image.open(p).convert(\"RGB\")\n        w, h = pil.size\n\n        # Get raw probability map\n        prob = segment_prob_map_with_tta(pil) if USE_TTA else segment_prob_map(pil)\n\n        # USE OLD MASK LOGIC: mean + 0.3*std\n        mask_raw, _ = enhanced_adaptive_mask(prob) # Your function using np.mean + 0.3*np.std\n        mask_resized = cv2.resize(mask_raw, (w, h), interpolation=cv2.INTER_NEAREST)\n\n        # Handle Ground Truth\n        if label == \"forged\":\n            m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n            if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n            m_gt = (m_gt > 0).astype(np.uint8)\n        else:\n            m_gt = np.zeros((h, w), np.uint8) # Authentic = blank GT\n\n        cache.append({\"prob\": prob, \"mask\": mask_resized, \"gt\": m_gt, \"label\": label})\n\n    # 2. Sweep thresholds\n    best_f1 = -1\n    best_params = {}\n    combinations = list(itertools.product(area_range, mean_range))\n\n    for a_thr, m_thr in combinations:\n        current_f1s = []\n        for item in cache:\n            mask = item[\"mask\"]\n            area = int(mask.sum()) # OLD AREA LOGIC\n\n            # OLD MEAN LOGIC\n            mask_small = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n            mean_in = float(item[\"prob\"][mask_small == 1].mean()) if area > 0 else 0.0\n\n            # Pipeline decision\n            is_forged = (area >= a_thr and mean_in >= m_thr)\n            m_pred = (mask > 0).astype(np.uint8) if is_forged else np.zeros_like(item[\"gt\"])\n\n            # F1 Calculation (Authentic silence = 1.0, noisy prediction = 0.0)\n            f1 = f1_score(item[\"gt\"].flatten(), m_pred.flatten(),\n                          zero_division=1 if item[\"label\"] == \"authentic\" else 0)\n            current_f1s.append(f1)\n\n        avg_f1 = np.mean(current_f1s)\n        if avg_f1 > best_f1:\n            best_f1 = avg_f1\n            best_params = {\"AREA_THR\": a_thr, \"MEAN_THR\": m_thr}\n            print(f\"‚≠ê New Best F1: {best_f1:.4f} -> AREA: {a_thr}, MEAN: {m_thr}\")\n\n    return best_params\n\nif GRID_SEARCH:\n    best_cfg = grid_search_area_mean(val_forg, val_auth, MASK_DIR)\n    AREA_THR = best_cfg['AREA_THR']\n    MEAN_THR = best_cfg['MEAN_THR']\n\n\nfrom sklearn.metrics import f1_score\nval_items = [(p, 1) for p in val_forg[:10]]\nresults = []\nfor p,_ in tqdm(val_items, desc=\"Validation forged-only\"):\n    pil = Image.open(p).convert(\"RGB\")\n    label, m_pred, dbg = pipeline_final(pil)\n    m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n    if m_gt.ndim==3: m_gt=np.max(m_gt,axis=0)\n    m_gt=(m_gt>0).astype(np.uint8)\n    m_pred=(m_pred>0).astype(np.uint8) if m_pred is not None else np.zeros_like(m_gt)\n    f1 = f1_score(m_gt.flatten(), m_pred.flatten(), zero_division=0)\n    results.append((Path(p).stem, f1, dbg))\nprint(\"\\n F1-score par image falsifi√©e:\\n\")\nfor cid,f1,dbg in results:\n    print(f\"{cid} ‚Äî F1={f1:.4f} | area={dbg['area']} mean={dbg['mean_inside']:.3f} thr={dbg['thr']:.3f}\")\nprint(f\"\\n Moyenne F1 (falsifi√©es) = {np.mean([r[1] for r in results]):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:37:48.075103Z","iopub.execute_input":"2026-01-10T07:37:48.075355Z","iopub.status.idle":"2026-01-10T07:37:55.684421Z","shell.execute_reply.started":"2026-01-10T07:37:48.075335Z","shell.execute_reply":"2026-01-10T07:37:55.683267Z"}},"outputs":[{"name":"stderr","text":"2026-01-10 07:37:51.420735: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768030671.443099     131 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768030671.449711     131 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768030671.467503     131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768030671.467530     131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768030671.467533     131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768030671.467535     131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_131/1931321192.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoImageProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDINO_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUSE_LOCAL_ONLY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDINO_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUSE_LOCAL_ONLY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDinoTinyDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4341\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4342\u001b[0m                 )\n\u001b[0;32m-> 4343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4345\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1367\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1353\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     )\n\u001b[0;32m-> 1355\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1356\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: uncorrectable ECC error encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"AcceleratorError","evalue":"CUDA error: uncorrectable ECC error encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"def _is_valid_rle_item(s: str) -> bool:\n    try:\n        arr = json.loads(s)\n        if not isinstance(arr, list):\n            return False\n        # Reject empty arrays explicitly\n        if len(arr) == 0:\n            return False\n        # Must be even length (start,length pairs)\n        if len(arr) % 2 != 0:\n            return False\n        starts = arr[0::2]\n        lengths = arr[1::2]\n        # Lengths must be positive\n        if any(int(l) <= 0 for l in lengths):\n            return False\n        # Starts must be in ascending order (1-based indices expected)\n        if sorted(starts) != list(starts):\n            return False\n        # Ensure no overlap within the same RLE sequence\n        # Convert to 0-based starts for consistency with decoder\n        starts0 = [int(s) - 1 for s in starts]\n        ends0 = [s0 + int(l) for s0, l in zip(starts0, lengths)]\n        for i in range(len(starts0) - 1):\n            if ends0[i] > starts0[i + 1]:\n                return False\n        return True\n    except Exception:\n        return False\n\ndef sanitize_annotation_str(s: str) -> str:\n    \"\"\"\n    Normalize a single annotation string:\n    - If 'authentic': keep as-is\n    - Else split by ';', drop empty/invalid JSON arrays, canonicalize JSON\n    - If nothing left, return 'authentic'\n    \"\"\"\n    s = str(s).strip()\n    if s.lower() == 'authentic':\n        return 'authentic'\n    parts = [p for p in s.split(';') if p.strip()]\n    keep = [json.dumps(json.loads(p)) for p in parts if _is_valid_rle_item(p)]\n    return ';'.join(keep) if keep else 'authentic'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:37:55.685130Z","iopub.status.idle":"2026-01-10T07:37:55.685390Z","shell.execute_reply.started":"2026-01-10T07:37:55.685271Z","shell.execute_reply":"2026-01-10T07:37:55.685286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os, json, cv2\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# --- RLE Encoder for Kaggle Submission ---\n# def rle_encode(mask: np.ndarray, fg_val: int = 1) -> str:\n#     pixels = mask.T.flatten()\n#     dots = np.where(pixels == fg_val)[0]\n#     if len(dots) == 0:\n#         return \"authentic\"\n#     run_lengths = []\n#     prev = -2\n#     for b in dots:\n#         if b > prev + 1:\n#             run_lengths.extend((b + 1, 0))\n#         run_lengths[-1] += 1\n#         prev = b\n#     return json.dumps([int(x) for x in run_lengths])\n\n# --- Paths (dynamic with BASE_DIR from earlier cell) ---\nTEST_DIR = f\"{BASE_DIR}/test_images\"\nSAMPLE_SUB = f\"{BASE_DIR}/sample_submission.csv\"\nOUT_PATH = \"submission.csv\"\n\nrows = []\nfor f in tqdm(sorted(os.listdir(TEST_DIR)), desc=\"Inference on Test Set\"):\n    pil = Image.open(Path(TEST_DIR)/f).convert(\"RGB\")\n    label, mask, dbg = pipeline_final(pil)  # utilise la version am√©lior√©e\n\n    # S√©curisation masque\n    if mask is None:\n        mask = np.zeros(pil.size[::-1], np.uint8)\n    else:\n        mask = np.array(mask, dtype=np.uint8)\n\n    # Annotation finale\n    if label == \"authentic\":\n        annot = \"authentic\"\n    else:\n        annot = rle_encode((mask > 0).astype(np.uint8))\n\n    rows.append({\n        \"case_id\": Path(f).stem,\n        \"annotation\": sanitize_annotation_str(annot),\n        \"area\": int(dbg.get(\"area\", mask.sum())),\n        \"mean\": float(dbg.get(\"mean_inside\", 0.0)),\n        \"thr\": float(dbg.get(\"thr\", 0.0))\n    })\n\n\nsub = pd.DataFrame(rows)\nss = pd.read_csv(SAMPLE_SUB)\nss[\"case_id\"] = ss[\"case_id\"].astype(str)\nsub[\"case_id\"] = sub[\"case_id\"].astype(str)\nfinal = ss[[\"case_id\"]].merge(sub, on=\"case_id\", how=\"left\")\nfinal[\"annotation\"] = final[\"annotation\"].fillna(\"authentic\")\nfinal[[\"case_id\", \"annotation\"]].to_csv(OUT_PATH, index=False)\n\nprint(f\"\\n‚úÖ Saved submission file: {OUT_PATH}\")\nprint(final.head(10))\n\n\nsample_files = sorted(os.listdir(TEST_DIR))[:5]\nfor f in sample_files:\n    pil = Image.open(Path(TEST_DIR)/f).convert(\"RGB\")\n    label, mask, dbg = pipeline_final(pil)\n    mask = np.array(mask, dtype=np.uint8) if mask is not None else np.zeros(pil.size[::-1], np.uint8)\n\n    print(f\"{'üî¥' if label=='forged' else 'üü¢'} {f}: {label} | area={mask.sum()} mean={dbg.get('mean_inside', 0):.3f}\")\n\n    if label == \"authentic\":\n        plt.figure(figsize=(5,5))\n        plt.imshow(pil)\n        plt.title(f\"{f} ‚Äî Authentic\")\n        plt.axis(\"off\")\n        plt.show()\n    else:\n        plt.figure(figsize=(10,5))\n        plt.subplot(1,2,1)\n        plt.imshow(pil)\n        plt.title(\"Original Image\")\n        plt.axis(\"off\")\n        plt.subplot(1,2,2)\n        plt.imshow(pil)\n        plt.imshow(mask, alpha=0.45, cmap=\"Reds\")\n        plt.title(f\"Predicted Forged Mask\\nArea={mask.sum()} | Mean={dbg.get('mean_inside', 0):.3f}\")\n        plt.axis(\"off\")\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:37:55.686529Z","iopub.status.idle":"2026-01-10T07:37:55.686803Z","shell.execute_reply.started":"2026-01-10T07:37:55.686681Z","shell.execute_reply":"2026-01-10T07:37:55.686696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final[\"annotation\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:37:55.687888Z","iopub.status.idle":"2026-01-10T07:37:55.688182Z","shell.execute_reply.started":"2026-01-10T07:37:55.688046Z","shell.execute_reply":"2026-01-10T07:37:55.688061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef validate_annotation(annotation: str):\n    # authentic „ÅØ„Åù„ÅÆ„Åæ„Åæ„ÅßOK\n    if annotation == \"authentic\":\n        return None\n    # „Çª„Éü„Ç≥„É≠„É≥Âå∫Âàá„Çä„ÅßÂêÑË¶ÅÁ¥†„ÅåÊ≠£„Åó„ÅÑJSONÈÖçÂàó„ÅãÊ§úË®º„Åó„ÄÅrle_decode„Åß„Åç„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ\n    parts = annotation.split(\";\")\n    if any(p.strip() == \"\" for p in parts):\n        return \"Annotation contains empty part or trailing semicolon.\"\n    try:\n        # ÂêÑ„Éë„Éº„ÉÑ„ÅØ JSON „ÅÆÊï∞ÂàóÔºàstart,length, start,length, ...Ôºâ\n        arrays = [np.asarray(json.loads(p), dtype=np.int32) for p in parts]\n    except Exception:\n        return \"Annotation parts are not valid JSON arrays.\"\n    # ÊòáÈ†Ü„ÉªÈáçË§á„Å™„Åó„Å™„Å©„ÅÆÊ§úË®º„ÅØ rle_decode „Å´Âßî„Å≠„Çã\n    try:\n        label_rles = [rle_decode(x, shape=(720, 960)) for x in annotation.split(';')]\n    except Exception:\n        raise \"decode error\"\n    # „Åì„Åì„Åß„ÅØÊúÄÂ∞èÈôê„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØÔºàÂÅ∂Êï∞Èï∑Ôºâ\n    for arr in arrays:\n        if len(arr) % 2 != 0:\n            return \"RLE array length must be even (start/length pairs).\"\n        starts = arr[0::2]\n        if sorted(starts.tolist()) != starts.tolist():\n            return \"RLE starts must be in ascending order.\"\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:37:55.689288Z","iopub.status.idle":"2026-01-10T07:37:55.689644Z","shell.execute_reply.started":"2026-01-10T07:37:55.689491Z","shell.execute_reply":"2026-01-10T07:37:55.689508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validate_annotation(final.annotation[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:37:55.690755Z","iopub.status.idle":"2026-01-10T07:37:55.691100Z","shell.execute_reply.started":"2026-01-10T07:37:55.690960Z","shell.execute_reply":"2026-01-10T07:37:55.690981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_rles = [rle_decode(x, shape=(720, 960)) for x in final.annotation[0].split(';')]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:37:55.691880Z","iopub.status.idle":"2026-01-10T07:37:55.692172Z","shell.execute_reply.started":"2026-01-10T07:37:55.692044Z","shell.execute_reply":"2026-01-10T07:37:55.692059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## supplemental debug","metadata":{}},{"cell_type":"code","source":"def gt_masks_from_npy(mask_path: Path) -> list[np.ndarray]:\n    m = np.load(mask_path)\n    if m.ndim == 3:\n        parts = [(m[k] > 0).astype(np.uint8) for k in range(m.shape[0])]\n        return parts\n    else:\n        return [((m > 0).astype(np.uint8))]\n\n# --- Validation using official oF1 on forged validation set ---\nprint(\"\\n--- Official oF1 Validation ---\")\n\nSUPPLEMENT_DIR  = f\"{BASE_DIR}/supplemental_images\"\nSUPPLE_MASK_DIR  = f\"{BASE_DIR}/supplemental_masks\"\nsupplemental_imgs = sorted([str(Path(SUPPLEMENT_DIR)/f) for f in os.listdir(SUPPLEMENT_DIR)])\nsupplemental_mask_imgs = sorted([str(Path(SUPPLE_MASK_DIR)/f) for f in os.listdir(SUPPLE_MASK_DIR)])\n\nval_items = supplemental_imgs\n\nscores = []\n\nfor p in tqdm(val_items, desc=\"oF1 forged-only\"):\n\n    pil = Image.open(p).convert(\"RGB\")\n    label, m_pred, dbg = pipeline_final(pil)\n\n    # Ground-truth masks (instance-aware)\n    gt_list = gt_masks_from_npy(Path(SUPPLE_MASK_DIR)/f\"{Path(p).stem}.npy\")\n\n    # Predicted masks list: empty if classified authentic; else single mask\n    if label == \"authentic\" or m_pred is None:\n        pred_list = []\n    else:\n        pred = (np.array(m_pred) > 0).astype(np.uint8)\n        pred_list = [pred]\n\n    # Compute official oF1\n    scores.append(oF1_score(pred_list, gt_list))\n\nprint(f\"Average oF1 (Forged Validation) = {np.mean(scores):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:37:55.693592Z","iopub.status.idle":"2026-01-10T07:37:55.693904Z","shell.execute_reply.started":"2026-01-10T07:37:55.693748Z","shell.execute_reply":"2026-01-10T07:37:55.693766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gt_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T07:37:55.695475Z","iopub.status.idle":"2026-01-10T07:37:55.695717Z","shell.execute_reply.started":"2026-01-10T07:37:55.695600Z","shell.execute_reply":"2026-01-10T07:37:55.695614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5gBcCXRU0AdO",
   "metadata": {
    "id": "5gBcCXRU0AdO"
   },
   "outputs": [],
   "source": [
    "# ãƒžã‚¦ãƒ³ãƒˆ\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a5694",
   "metadata": {
    "id": "cf7a5694"
   },
   "outputs": [],
   "source": [
    "!uv pip install kaggle\n",
    "!mkdir /root/.kaggle\n",
    "!cp /content/drive/MyDrive/kaggle/kaggle.json /root/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda0498a",
   "metadata": {
    "id": "fda0498a"
   },
   "outputs": [],
   "source": [
    "!kaggle competitions download -c recodai-luc-scientific-image-forgery-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63219d32",
   "metadata": {
    "id": "63219d32"
   },
   "outputs": [],
   "source": [
    "!unzip -o /content/recodai-luc-scientific-image-forgery-detection.zip -d input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ad872",
   "metadata": {
    "id": "685ad872"
   },
   "outputs": [],
   "source": [
    "!uv pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde13f5",
   "metadata": {
    "id": "dcde13f5"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from google.colab import userdata\n",
    "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
    "!wandb login $wandb_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vFooF4n4Ghve",
   "metadata": {
    "id": "vFooF4n4Ghve"
   },
   "source": [
    "## metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-hlcQmfhGoGA",
   "metadata": {
    "id": "-hlcQmfhGoGA"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "from numba import types\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import scipy.optimize\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def _rle_encode_jit(x: npt.NDArray, fg_val: int = 1) -> list[int]|str:\n",
    "    \"\"\"Numba-jitted RLE encoder.\"\"\"\n",
    "    dots = np.where(x.T.flatten() == fg_val)[0]\n",
    "    if len(dots) == 0:\n",
    "        return \"authentic\"\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if b > prev + 1:\n",
    "            run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "def rle_encode(masks: list[npt.NDArray], fg_val: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Adapted from contrails RLE https://www.kaggle.com/code/inversion/contrails-rle-submission\n",
    "    Args:\n",
    "        masks: list of numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns: run length encodings as a string, with each RLE JSON-encoded and separated by a semicolon.\n",
    "    \"\"\"\n",
    "    return ';'.join([json.dumps(_rle_encode_jit(x, fg_val)) for x in masks])\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def _rle_decode_jit(mask_rle: npt.NDArray, height: int, width: int) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    s: numpy array of run-length encoding pairs (start, length)\n",
    "    shape: (height, width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    \"\"\"\n",
    "    if len(mask_rle) % 2 != 0:\n",
    "        # Numba requires raising a standard exception.\n",
    "        raise ValueError('One or more rows has an odd number of values.')\n",
    "\n",
    "    starts, lengths = mask_rle[0::2], mask_rle[1::2]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    for i in range(len(starts) - 1):\n",
    "        if ends[i] > starts[i + 1]:\n",
    "            raise ValueError('Pixels must not be overlapping.')\n",
    "    img = np.zeros(height * width, dtype=np.bool_)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img\n",
    "\n",
    "\n",
    "def rle_decode(mask_rle: str, shape: tuple[int, int]) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    mask_rle: run-length as string formatted (start length)\n",
    "              empty predictions need to be encoded with '-'\n",
    "    shape: (height, width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    \"\"\"\n",
    "\n",
    "    mask_rle = json.loads(mask_rle)\n",
    "    mask_rle = np.asarray(mask_rle, dtype=np.int32)\n",
    "    starts = mask_rle[0::2]\n",
    "    if sorted(starts) != list(starts):\n",
    "        raise ParticipantVisibleError('Submitted values must be in ascending order.')\n",
    "    try:\n",
    "        return _rle_decode_jit(mask_rle, shape[0], shape[1]).reshape(shape, order='F')\n",
    "    except ValueError as e:\n",
    "        raise ParticipantVisibleError(str(e)) from e\n",
    "\n",
    "\n",
    "def calculate_f1_score(pred_mask: npt.NDArray, gt_mask: npt.NDArray):\n",
    "    pred_flat = pred_mask.flatten()\n",
    "    gt_flat = gt_mask.flatten()\n",
    "\n",
    "    tp = np.sum((pred_flat == 1) & (gt_flat == 1))\n",
    "    fp = np.sum((pred_flat == 1) & (gt_flat == 0))\n",
    "    fn = np.sum((pred_flat == 0) & (gt_flat == 1))\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    if (precision + recall) > 0:\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def calculate_f1_matrix(pred_masks: list[npt.NDArray], gt_masks: list[npt.NDArray]):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    pred_masks (np.ndarray):\n",
    "            First dimension is the number of predicted instances.\n",
    "            Each instance is a binary mask of shape (height, width).\n",
    "    gt_masks (np.ndarray):\n",
    "            First dimension is the number of ground truth instances.\n",
    "            Each instance is a binary mask of shape (height, width).\n",
    "    \"\"\"\n",
    "\n",
    "    num_instances_pred = len(pred_masks)\n",
    "    num_instances_gt = len(gt_masks)\n",
    "    f1_matrix = np.zeros((num_instances_pred, num_instances_gt))\n",
    "\n",
    "    # Calculate F1 scores for each pair of predicted and ground truth masks\n",
    "    for i in range(num_instances_pred):\n",
    "        for j in range(num_instances_gt):\n",
    "            pred_flat = pred_masks[i].flatten()\n",
    "            gt_flat = gt_masks[j].flatten()\n",
    "            f1_matrix[i, j] = calculate_f1_score(pred_mask=pred_flat, gt_mask=gt_flat)\n",
    "\n",
    "    if f1_matrix.shape[0] < len(gt_masks):\n",
    "        # Add a row of zeros to the matrix if the number of predicted instances is less than ground truth instances\n",
    "        f1_matrix = np.vstack((f1_matrix, np.zeros((len(gt_masks) - len(f1_matrix), num_instances_gt))))\n",
    "\n",
    "    return f1_matrix\n",
    "\n",
    "\n",
    "def oF1_score(pred_masks: list[npt.NDArray], gt_masks: list[npt.NDArray]):\n",
    "    \"\"\"\n",
    "    Calculate the optimal F1 score for a set of predicted masks against\n",
    "    ground truth masks which considers the optimal F1 score matching.\n",
    "    This function uses the Hungarian algorithm to find the optimal assignment\n",
    "    of predicted masks to ground truth masks based on the F1 score matrix.\n",
    "    If the number of predicted masks is less than the number of ground truth masks,\n",
    "    it will add a row of zeros to the F1 score matrix to ensure that the dimensions match.\n",
    "\n",
    "    Parameters:\n",
    "    pred_masks (list of np.ndarray): List of predicted binary masks.\n",
    "    gt_masks (np.ndarray): Array of ground truth binary masks.\n",
    "    Returns:\n",
    "    float: Optimal F1 score.\n",
    "    \"\"\"\n",
    "    f1_matrix = calculate_f1_matrix(pred_masks, gt_masks)\n",
    "\n",
    "    # Find the best matching between predicted and ground truth masks\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(-f1_matrix)\n",
    "    # The linear_sum_assignment discards excess predictions so we need a separate penalty.\n",
    "    excess_predictions_penalty = len(gt_masks) / max(len(pred_masks), len(gt_masks))\n",
    "    return np.mean(f1_matrix[row_ind, col_ind]) * excess_predictions_penalty\n",
    "\n",
    "\n",
    "def evaluate_single_image(label_rles: str, prediction_rles: str, shape_str: str) -> float:\n",
    "    shape = json.loads(shape_str)\n",
    "    label_rles = [rle_decode(x, shape=shape) for x in label_rles.split(';')]\n",
    "    prediction_rles = [rle_decode(x, shape=shape) for x in prediction_rles.split(';')]\n",
    "    return oF1_score(prediction_rles, label_rles)\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        solution (pd.DataFrame): The ground truth DataFrame.\n",
    "        submission (pd.DataFrame): The submission DataFrame.\n",
    "        row_id_column_name (str): The name of the column containing row IDs.\n",
    "    Returns:\n",
    "        float\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['authentic', 'authentic', 'authentic'], 'shape': ['authentic', 'authentic', 'authentic']})\n",
    "    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['authentic', 'authentic', 'authentic']})\n",
    "    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n",
    "    1.0\n",
    "\n",
    "    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['authentic', 'authentic', 'authentic'], 'shape': ['authentic', 'authentic', 'authentic']})\n",
    "    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]']})\n",
    "    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n",
    "    0.0\n",
    "\n",
    "    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]'], 'shape': ['[720, 960]', '[720, 960]', '[720, 960]']})\n",
    "    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]']})\n",
    "    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n",
    "    1.0\n",
    "\n",
    "    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 103]', '[101, 102]', '[101, 102]'], 'shape': ['[720, 960]', '[720, 960]', '[720, 960]']})\n",
    "    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]']})\n",
    "    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n",
    "    0.9983739837398374\n",
    "\n",
    "    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102];[300, 100]', '[101, 102]', '[101, 102]'], 'shape': ['[720, 960]', '[720, 960]', '[720, 960]']})\n",
    "    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]']})\n",
    "    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n",
    "    0.8333333333333334\n",
    "    \"\"\"\n",
    "    df = solution\n",
    "    df = df.rename(columns={'annotation': 'label'})\n",
    "\n",
    "    df['prediction'] = submission['annotation']\n",
    "    # Check for correct 'authentic' label\n",
    "    authentic_indices = (df['label'] == 'authentic') | (df['prediction'] == 'authentic')\n",
    "    df['image_score'] = ((df['label'] == df['prediction']) & authentic_indices).astype(float)\n",
    "\n",
    "    df.loc[~authentic_indices, 'image_score'] = df.loc[~authentic_indices].apply(\n",
    "        lambda row: evaluate_single_image(row['label'], row['prediction'], row['shape']), axis=1\n",
    "    )\n",
    "    return float(np.mean(df['image_score']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c303e71",
   "metadata": {
    "id": "7c303e71",
    "papermill": {
     "duration": 0.003736,
     "end_time": "2026-01-05T16:07:44.266168",
     "exception": false,
     "start_time": "2026-01-05T16:07:44.262432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Notebook Overview: CNN-DINOv2 Hybrid\n",
    "\n",
    "This notebook demonstrates a hybrid approach for image classification using both Convolutional Neural Networks (CNNs) and DINOv2, a self-supervised vision transformer model. The workflow includes:\n",
    "\n",
    "- **Data Loading & Preprocessing:** Images are loaded, resized, normalized, and split into training and validation sets.\n",
    "- **Feature Extraction:** DINOv2 is used to extract high-level features from images, leveraging its transformer-based architecture for robust representations.\n",
    "- **CNN Model Construction:** A custom CNN is built to process image data, learning spatial hierarchies and patterns.\n",
    "- **Hybrid Model Integration:** Features from DINOv2 and the CNN are combined, either by concatenation or other fusion techniques, to enhance classification performance.\n",
    "- **Training & Evaluation:** The hybrid model is trained on the dataset, with metrics such as accuracy and loss tracked. Validation is performed to assess generalization.\n",
    "- **Visualization & Analysis:** Results, including confusion matrices and sample predictions, are visualized to interpret model behavior.\n",
    "\n",
    "This approach aims to leverage the strengths of both CNNs (local feature learning) and DINOv2 (global, context-aware representations) for improved image classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a95564",
   "metadata": {
    "id": "35a95564",
    "papermill": {
     "duration": 0.00269,
     "end_time": "2026-01-05T16:07:44.271807",
     "exception": false,
     "start_time": "2026-01-05T16:07:44.269117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Step 1: Model Setup, Dataset Preparation, and Validation Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c2e39",
   "metadata": {
    "id": "e53c2e39"
   },
   "outputs": [],
   "source": [
    "# --- Transformer-based DinoMFDecoder (Mask2Former-style, single-scale minimal) ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "class _MaskDecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One decoder stage composed of CA (cross-attention), SA (self-attention), and FFN.\n",
    "    - CA: queries attend to pixel tokens (image features)\n",
    "    - SA: queries attend to themselves to resolve mutual relations\n",
    "    - FFN: position-wise feed-forward for non-linearity\n",
    "    Order here follows the diagram: CA -> SA -> FFN.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.self_attn  = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.linear1    = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2    = nn.Linear(dim_feedforward, d_model)\n",
    "        self.dropout    = nn.Dropout(dropout)\n",
    "        self.dropout1   = nn.Dropout(dropout)\n",
    "        self.dropout2   = nn.Dropout(dropout)\n",
    "        self.norm1      = nn.LayerNorm(d_model)\n",
    "        self.norm2      = nn.LayerNorm(d_model)\n",
    "        self.norm3      = nn.LayerNorm(d_model)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, kv: torch.Tensor, kv_pos: torch.Tensor = None):\n",
    "        # Cross-Attention (queries <- pixels)\n",
    "        k = v = kv if kv_pos is None else kv + kv_pos\n",
    "        q2, _ = self.cross_attn(query=q, key=k, value=v, need_weights=False)\n",
    "        q = q + self.dropout1(q2)\n",
    "        q = self.norm1(q)\n",
    "        # Self-Attention (queries <- queries)\n",
    "        q2, _ = self.self_attn(query=q, key=q, value=q, need_weights=False)\n",
    "        q = q + self.dropout2(q2)\n",
    "        q = self.norm2(q)\n",
    "        # FFN\n",
    "        q2 = self.linear2(self.dropout(self.activation(self.linear1(q))))\n",
    "        q = self.norm3(q + q2)\n",
    "        return q\n",
    "\n",
    "class DinoMFDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Mask2Former-like minimal decoder:\n",
    "    - Projects DINOv2 fmap (B,C,h,w) -> (B,D,h,w) as pixel embeddings\n",
    "    - Uses learnable queries (Q,D)\n",
    "    - L stages of [CA -> SA -> FFN]\n",
    "    - Produces Q masks via dot-product between query mask-embeds and pixel embeddings\n",
    "    Returns a single-channel mask by max-pooling across queries (compatible with existing pipeline).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_ch: int = 768,\n",
    "        out_ch: int = 1,\n",
    "        d_model: int = 256,\n",
    "        nhead: int = 8,\n",
    "        num_queries: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 1024,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pixel_proj = nn.Conv2d(in_ch, d_model, kernel_size=1)\n",
    "        self.query_embed = nn.Embedding(num_queries, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            _MaskDecoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.mask_embed = nn.Linear(d_model, d_model)\n",
    "        self.class_embed = nn.Linear(d_model, 2)  # forged / void (optional)\n",
    "        # simple 2D sine-cos positional encoding for pixels\n",
    "        self.register_buffer(\"pe_cache\", None, persistent=False)\n",
    "\n",
    "    def _pos2d(self, H: int, W: int, device) -> torch.Tensor:\n",
    "        # cached for a given HxW\n",
    "        if self.pe_cache is not None and self.pe_cache.shape[1] == H*W:\n",
    "            return self.pe_cache\n",
    "        y, x = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing=\"ij\")\n",
    "        omega = torch.arange(self.d_model // 4, device=device, dtype=torch.float32) / (self.d_model // 4)\n",
    "        omega = 1.0 / (10000 ** omega)\n",
    "        out_y = torch.cat([torch.sin(y.reshape(-1,1) * omega), torch.cos(y.reshape(-1,1) * omega)], dim=1)\n",
    "        out_x = torch.cat([torch.sin(x.reshape(-1,1) * omega), torch.cos(x.reshape(-1,1) * omega)], dim=1)\n",
    "        pe = torch.cat([out_y, out_x], dim=1)\n",
    "        if pe.shape[1] < self.d_model:\n",
    "            pe = F.pad(pe, (0, self.d_model - pe.shape[1]))\n",
    "        else:\n",
    "            pe = pe[:, :self.d_model]\n",
    "        pe = pe.unsqueeze(0)  # (1, HW, D)\n",
    "        self.pe_cache = pe\n",
    "        return pe\n",
    "\n",
    "    def forward(self, f: torch.Tensor, target_size: Tuple[int, int]):\n",
    "        # f: (B, C, h, w)\n",
    "        B, C, h, w = f.shape\n",
    "        device = f.device\n",
    "        pix = self.pixel_proj(f)                 # (B, D, h, w)\n",
    "        pix_flat = pix.flatten(2).permute(0, 2, 1)  # (B, HW, D)\n",
    "        pos = self._pos2d(h, w, device).expand(B, -1, -1)  # (B, HW, D)\n",
    "\n",
    "        # prepare queries\n",
    "        q = self.query_embed.weight.unsqueeze(0).expand(B, -1, -1)  # (B, Q, D)\n",
    "        for layer in self.layers:\n",
    "            q = layer(q, kv=pix_flat, kv_pos=pos)\n",
    "\n",
    "        # class logits (optional, not used downstream yet)\n",
    "        cls_logits = self.class_embed(q)  # (B, Q, 2)\n",
    "\n",
    "        # mask logits via dot product between query mask-embed and pixel embeddings\n",
    "        mask_tokens = self.mask_embed(q)              # (B, Q, D)\n",
    "        pix_grid = pix                               # (B, D, h, w)\n",
    "        mask_logits = torch.einsum('bqd,bdhw->bqhw', mask_tokens, pix_grid)  # (B, Q, h, w)\n",
    "\n",
    "        # collapse queries by max (keep old interface: out_ch=1)\n",
    "        logits_1 = mask_logits.max(dim=1, keepdim=True).values  # (B,1,h,w)\n",
    "        logits_1 = F.interpolate(logits_1, size=target_size, mode='bilinear', align_corners=False)\n",
    "        return logits_1\n",
    "\n",
    "# Usage note:\n",
    "# - To switch: model_seg.seg_head = DinoMFDecoder(768, 1, d_model=256, nhead=8, num_queries=64, num_layers=2)\n",
    "# - This keeps the same forward_seg signature and downstream pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aYF8_Hi2-oei",
   "metadata": {
    "id": "aYF8_Hi2-oei"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38d612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T16:07:44.279739Z",
     "iopub.status.busy": "2026-01-05T16:07:44.279346Z",
     "iopub.status.idle": "2026-01-05T16:09:17.608101Z",
     "shell.execute_reply": "2026-01-05T16:09:17.607071Z"
    },
    "id": "dd38d612",
    "papermill": {
     "duration": 93.336454,
     "end_time": "2026-01-05T16:09:17.610875",
     "exception": false,
     "start_time": "2026-01-05T16:07:44.274421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, cv2, json, math, random, torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # This forces CUDA to use deterministic algorithms (slower but consistent)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Environment detection & dynamic paths (Kaggle vs Colab) ---\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input/recodai-luc-scientific-image-forgery-detection')\n",
    "IS_COLAB = 'google.colab' in globals() or 'COLAB_GPU' in os.environ\n",
    "\n",
    "BASE_DIR  = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\" if IS_KAGGLE else \"/content/input/\"\n",
    "\n",
    "AUTH_DIR  = f\"{BASE_DIR}/train_images/authentic\"\n",
    "FORG_DIR  = f\"{BASE_DIR}/train_images/forged\"\n",
    "MASK_DIR  = f\"{BASE_DIR}/train_masks\"\n",
    "TEST_DIR  = f\"{BASE_DIR}/test_images\"\n",
    "\n",
    "# DINO model reference: Kaggle dataset path or Hugging Face ID for Colab\n",
    "DINO_PATH = \"/kaggle/input/dinov2/pytorch/base/1\" if IS_KAGGLE else os.environ.get(\"DINO_ID\", \"facebook/dinov2-base\")\n",
    "\n",
    "# On Kaggle, prefer local files only; on Colab, allow remote download\n",
    "USE_LOCAL_ONLY = True if IS_KAGGLE else False\n",
    "\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 2\n",
    "# MODEL_LOC examples for Kaggle (optional). In Colab, set to None or a local path under /content.\n",
    "# MODEL_LOC = '/kaggle/input/cnndinov2-pbd/CNNDINOv2-U54/CNNDINOv2-U54/model_seg_final.pt'  # 0.310\n",
    "# MODEL_LOC = '/kaggle/input/cnndinov2-pbd/CNNDINOv2-U52/CNNDINOv2-U52/model_seg_final.pt'  # 0.321\n",
    "MODEL_LOC = '/kaggle/input/cnndinov2-pbd/CNNDINOv2-U52/CNNDINOv2-U52/model_seg_final.pt' if IS_KAGGLE else None\n",
    "EPOCHS_SEG = 4\n",
    "LR_SEG = 1e-5\n",
    "WEIGHT_DECAY = 2e-5\n",
    "VAL_SAMPLES = 10 # validate on a small forged subset per epoch\n",
    "\n",
    "# INFERENCE UTILS\n",
    "AREA_THR = 200\n",
    "MEAN_THR = 0.22\n",
    "USE_TTA = False\n",
    "GRID_SEARCH = False\n",
    "\n",
    "class ForgerySegDataset(Dataset):\n",
    "    def __init__(self, auth_paths, forg_paths, mask_dir, img_size=IMG_SIZE):\n",
    "        self.samples = []\n",
    "        for p in forg_paths:\n",
    "            m = os.path.join(mask_dir, Path(p).stem + \".npy\")\n",
    "            if os.path.exists(m):\n",
    "                self.samples.append((p, m))\n",
    "        for p in auth_paths:\n",
    "            self.samples.append((p, None))\n",
    "        self.img_size = img_size\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "        if mask_path is None:\n",
    "            mask = np.zeros((h, w), np.uint8)\n",
    "        else:\n",
    "            m = np.load(mask_path)\n",
    "            if m.ndim == 3: m = np.max(m, axis=0)\n",
    "            mask = (m > 0).astype(np.uint8)\n",
    "        img_r = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "        mask_r = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "        img_t = torch.from_numpy(np.array(img_r, np.float32)/255.).permute(2,0,1)\n",
    "        mask_t = torch.from_numpy(mask_r[None, ...].astype(np.float32))\n",
    "        return img_t, mask_t\n",
    "\n",
    "\n",
    "#  MODEL (DINOv2 + Decoder)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(DINO_PATH, local_files_only=USE_LOCAL_ONLY, use_fast=False)\n",
    "encoder = AutoModel.from_pretrained(DINO_PATH, local_files_only=USE_LOCAL_ONLY).eval().to(device)\n",
    "\n",
    "class DinoTinyDecoder(nn.Module):\n",
    "    def __init__(self, in_ch=768, out_ch=1):\n",
    "        super().__init__()\n",
    "        # Block 1: 768 -> 384\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1)\n",
    "        )\n",
    "        # Block 2: 384 -> 192\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(384, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1)\n",
    "        )\n",
    "        # Block 3: 192 -> 96\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(192, 96, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Final Output: 96 -> 1\n",
    "        self.conv_out = nn.Conv2d(96, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, f, target_size):\n",
    "        # f: [B, 768, 37, 37]\n",
    "\n",
    "        # Step 1: Up to ~74x74\n",
    "        x = F.interpolate(self.block1(f), size=(74, 74), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Step 2: Up to ~148x148\n",
    "        x = F.interpolate(self.block2(x), size=(148, 148), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Step 3: Up to ~296x296\n",
    "        x = F.interpolate(self.block3(x), size=(296, 296), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Step 4: Final jump to 518x518\n",
    "        x = self.conv_out(x)\n",
    "        x = F.interpolate(x, size=target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DinoSegmenter(nn.Module):\n",
    "    def __init__(self, encoder, processor):\n",
    "        super().__init__()\n",
    "        self.encoder, self.processor = encoder, processor\n",
    "        for p in self.encoder.parameters(): p.requires_grad = False\n",
    "        self.seg_head = DinoMFDecoder(768,1)\n",
    "    def forward_features(self,x):\n",
    "        imgs = (x*255).clamp(0,255).byte().permute(0,2,3,1).cpu().numpy()\n",
    "        inputs = self.processor(images=list(imgs), return_tensors=\"pt\").to(x.device)\n",
    "        # with torch.no_grad():\n",
    "        #     feats = self.encoder(**inputs).last_hidden_state\n",
    "        feats = self.encoder(**inputs).last_hidden_state\n",
    "        B,N,C = feats.shape\n",
    "        fmap = feats[:,1:,:].permute(0,2,1)\n",
    "        s = int(math.sqrt(N-1))\n",
    "        fmap = fmap.reshape(B,C,s,s)\n",
    "        return fmap\n",
    "    def forward_seg(self,x):\n",
    "        fmap = self.forward_features(x)\n",
    "        return self.seg_head(fmap,(IMG_SIZE,IMG_SIZE))\n",
    "\n",
    "# --- 3.1 Utilities: init bias, losses, small eval ---\n",
    "\n",
    "def init_last_conv_bias_negative(module: nn.Module, bias_value: float = -4.0):\n",
    "    \"\"\"Initialize the final 1-channel Conv2d bias to a negative value to avoid early saturation.\"\"\"\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d) and m.out_channels == 1:\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, bias_value)\n",
    "\n",
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        targets = targets.float()\n",
    "        inter = (probs * targets).sum(dim=(2,3))\n",
    "        denom = probs.sum(dim=(2,3)) + targets.sum(dim=(2,3)) + self.eps\n",
    "        dice = (2 * inter + self.eps) / denom\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "def enhanced_adaptive_mask(prob, alpha_grad=0.45):\n",
    "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
    "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
    "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
    "    enhanced = cv2.GaussianBlur(enhanced, (3,3), 0)\n",
    "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
    "    mask = (enhanced > thr).astype(np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
    "    return mask, thr\n",
    "\n",
    "def finalize_mask(prob, orig_size):\n",
    "    mask, thr = enhanced_adaptive_mask(prob)\n",
    "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
    "    return mask, thr\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_mean_f1(model_seg, val_list, mask_dir, max_items=VAL_SAMPLES):\n",
    "    model_seg.eval()\n",
    "    items = val_list[:max_items]\n",
    "    f1s = []\n",
    "    for p in items:\n",
    "        pil = Image.open(p).convert(\"RGB\")\n",
    "        x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
    "        logits = model_seg.forward_seg(x)\n",
    "        prob = torch.sigmoid(logits)[0,0].cpu().numpy()\n",
    "        m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n",
    "        if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
    "        m_gt = (m_gt > 0).astype(np.uint8)\n",
    "        mask, _ = finalize_mask(prob, (m_gt.shape[1], m_gt.shape[0]))\n",
    "        f1s.append(f1_score(m_gt.flatten(), mask.flatten(), zero_division=0))\n",
    "    return float(np.mean(f1s)) if f1s else 0.0\n",
    "\n",
    "# --- oF1 (Optimal F1) utilities aligned with official metric ---\n",
    "def split_instances(mask: np.ndarray):\n",
    "    mask = (mask > 0).astype(np.uint8)\n",
    "    num, labels = cv2.connectedComponents(mask)\n",
    "    instances = []\n",
    "    for k in range(1, num):\n",
    "        inst = (labels == k).astype(np.uint8)\n",
    "        if inst.sum() > 0:\n",
    "            instances.append(inst)\n",
    "    return instances\n",
    "\n",
    "def f1_pixels(gt: np.ndarray, pred: np.ndarray):\n",
    "    gt = (gt > 0).astype(np.uint8)\n",
    "    pred = (pred > 0).astype(np.uint8)\n",
    "    tp = int((gt & pred).sum())\n",
    "    fp = int((pred & (1 - gt)).sum())\n",
    "    fn = int(((1 - pred) & gt).sum())\n",
    "    denom = 2 * tp + fp + fn\n",
    "    return 0.0 if denom == 0 else (2.0 * tp) / float(denom)\n",
    "\n",
    "def optimal_f1(gt_mask: np.ndarray, pred_mask: np.ndarray):\n",
    "    gt_insts = split_instances(gt_mask)\n",
    "    pred_insts = split_instances(pred_mask)\n",
    "    if len(gt_insts) == 0 and len(pred_insts) == 0:\n",
    "        return 1.0\n",
    "    if len(gt_insts) == 0 and len(pred_insts) > 0:\n",
    "        return 0.0\n",
    "    if len(gt_insts) > 0 and len(pred_insts) == 0:\n",
    "        return 0.0\n",
    "    M = np.zeros((len(gt_insts), len(pred_insts)), dtype=np.float32)\n",
    "    for i, g in enumerate(gt_insts):\n",
    "        for j, p in enumerate(pred_insts):\n",
    "            M[i, j] = f1_pixels(g, p)\n",
    "    matched_mean = 0.0\n",
    "    try:\n",
    "        from scipy.optimize import linear_sum_assignment\n",
    "        row_ind, col_ind = linear_sum_assignment(1.0 - M)\n",
    "        matched_mean = M[row_ind, col_ind].mean() if len(row_ind) > 0 else 0.0\n",
    "    except Exception:\n",
    "        used_cols = set()\n",
    "        acc = 0.0\n",
    "        cnt = 0\n",
    "        for i in range(len(gt_insts)):\n",
    "            j = int(np.argmax(M[i]))\n",
    "            if j not in used_cols:\n",
    "                acc += float(M[i, j])\n",
    "                used_cols.add(j)\n",
    "                cnt += 1\n",
    "        matched_mean = acc / max(1, cnt)\n",
    "    penalty = len(gt_insts) / float(max(len(gt_insts), len(pred_insts)))\n",
    "    return float(matched_mean * penalty)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_mean_oF1(model_seg, val_list, mask_dir, max_items=VAL_SAMPLES):\n",
    "    model_seg.eval()\n",
    "    items = val_list[:max_items]\n",
    "    scores = []\n",
    "    for p in items:\n",
    "        pil = Image.open(p).convert(\"RGB\")\n",
    "        x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
    "        logits = model_seg.forward_seg(x)\n",
    "        prob = torch.sigmoid(logits)[0,0].cpu().numpy()\n",
    "        m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n",
    "        if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
    "        m_gt = (m_gt > 0).astype(np.uint8)\n",
    "        mask, _ = finalize_mask(prob, (m_gt.shape[1], m_gt.shape[0]))\n",
    "        scores.append(optimal_f1(m_gt, mask))\n",
    "    return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "# --- 4. DATA SPLIT AND TRAINING ---\n",
    "auth_imgs = sorted([str(Path(AUTH_DIR)/f) for f in os.listdir(AUTH_DIR)])\n",
    "forg_imgs = sorted([str(Path(FORG_DIR)/f) for f in os.listdir(FORG_DIR)])\n",
    "train_auth, val_auth = train_test_split(auth_imgs, test_size=0.2, random_state=42)\n",
    "train_forg, val_forg = train_test_split(forg_imgs, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(ForgerySegDataset(train_auth, train_forg, MASK_DIR),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(ForgerySegDataset(val_auth, val_forg, MASK_DIR),\n",
    "                        batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "model_seg = DinoSegmenter(encoder, processor).to(device)\n",
    "\n",
    "init_last_conv_bias_negative(model_seg.seg_head, bias_value=-4.0)\n",
    "\n",
    "# Train only decoder\n",
    "opt_seg = optim.AdamW(model_seg.seg_head.parameters(), lr=LR_SEG, weight_decay=WEIGHT_DECAY)\n",
    "crit_bce = nn.BCEWithLogitsLoss()\n",
    "crit_dice = SoftDiceLoss()\n",
    "\n",
    "def seg_loss(logits, targets, alpha=0.5):\n",
    "    return alpha * crit_bce(logits, targets) + (1 - alpha) * crit_dice(logits, targets)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "def train_seg(epoch):\n",
    "    best_f1 = -1.0\n",
    "    best_path = \"model_seg_best.pt\"\n",
    "\n",
    "    for e in range(epoch):\n",
    "        model_seg.train()\n",
    "        model_seg.encoder.eval()  # keep encoder frozen/eval\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x, m in tqdm(train_loader, desc=f\"[Segmentation] Epoch {e+1}/{epoch}\"):\n",
    "            x, m = x.to(device), m.to(device)\n",
    "            opt_seg.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                logits = model_seg.forward_seg(x)\n",
    "                loss = seg_loss(logits, m)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt_seg)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / max(1, len(train_loader))\n",
    "\n",
    "        # quick validation on forged subset (official oF1)\n",
    "        val_f1 = evaluate_mean_oF1(model_seg, val_forg, MASK_DIR, max_items=VAL_SAMPLES)\n",
    "        print(f\"  â†’ avg_loss={avg_loss:.4f} | val_oF1@{VAL_SAMPLES}={val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            torch.save(model_seg.state_dict(), best_path)\n",
    "            print(f\"  âœ… New best saved to {best_path}\")\n",
    "\n",
    "    # Save final model state\n",
    "    torch.save(model_seg.state_dict(), \"model_seg_final.pt\")\n",
    "    print(\"Model saved as model_seg_final.pt\")\n",
    "\n",
    "\n",
    " # Load pretrained weights if MODEL_LOC is specified\n",
    "if MODEL_LOC is not None and os.path.exists(MODEL_LOC):\n",
    "    model_seg.load_state_dict(torch.load(MODEL_LOC, map_location=device))\n",
    "    print(f\"âœ… Loaded pretrained model from: {MODEL_LOC}\")\n",
    "    model_seg.eval()  # Set model to evaluation mode\n",
    "else:\n",
    "    print(\"model not found, training start!\")\n",
    "    train_seg(EPOCHS_SEG)\n",
    "\n",
    "@torch.no_grad()\n",
    "def segment_prob_map(pil):\n",
    "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
    "    prob = torch.sigmoid(model_seg.forward_seg(x))[0,0].cpu().numpy()\n",
    "    return prob\n",
    "\n",
    "@torch.no_grad()\n",
    "def segment_prob_map_with_tta(pil):\n",
    "    # 1. Preprocessing: Resize, Normalize, and move to Device\n",
    "    x = torch.from_numpy(np.array(pil.resize((IMG_SIZE, IMG_SIZE)), np.float32)/255.).permute(2,0,1)[None].to(device)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # 2. Original Prediction\n",
    "    pred_orig = torch.sigmoid(model_seg.forward_seg(x))\n",
    "    predictions.append(pred_orig)\n",
    "\n",
    "    # 3. Horizontal Flip TTA (dim 3)\n",
    "    # Flip input -> Predict -> Flip output back\n",
    "    pred_h = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[3])))\n",
    "    predictions.append(torch.flip(pred_h, dims=[3]))\n",
    "\n",
    "    # 4. Vertical Flip TTA (dim 2)\n",
    "    # Flip input -> Predict -> Flip output back\n",
    "    pred_v = torch.sigmoid(model_seg.forward_seg(torch.flip(x, dims=[2])))\n",
    "    predictions.append(torch.flip(pred_v, dims=[2]))\n",
    "\n",
    "    # 5. Average the predictions and format as numpy\n",
    "    # We stack the 3 predictions and take the mean across the stack dimension (0)\n",
    "    prob = torch.stack(predictions).mean(0)[0, 0].cpu().numpy()\n",
    "\n",
    "    return prob\n",
    "\n",
    "def finalize_mask(prob, orig_size):\n",
    "    mask, thr = enhanced_adaptive_mask(prob)\n",
    "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
    "    return mask, thr\n",
    "\n",
    "def pipeline_final(pil):\n",
    "    if USE_TTA:\n",
    "        prob = segment_prob_map_with_tta(pil)\n",
    "    else:\n",
    "        prob = segment_prob_map(pil)\n",
    "    mask, thr = finalize_mask(prob, pil.size)\n",
    "    area = int(mask.sum())\n",
    "    mean_inside = float(prob[cv2.resize(mask,(IMG_SIZE,IMG_SIZE),interpolation=cv2.INTER_NEAREST)==1].mean()) if area>0 else 0.0\n",
    "    if area < AREA_THR or mean_inside < MEAN_THR:\n",
    "        return \"authentic\", None, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
    "    return \"forged\", mask, {\"area\": area, \"mean_inside\": mean_inside, \"thr\": thr}\n",
    "\n",
    "import itertools\n",
    "\n",
    "def grid_search_area_mean(forg_paths, auth_paths, mask_dir):\n",
    "    mean_range = [round(x, 2) for x in np.arange(0.20, 0.291, 0.01)]\n",
    "    area_range = [200]\n",
    "    val_set = [(p, \"forged\") for p in forg_paths] + [(p, \"authentic\") for p in auth_paths]\n",
    "\n",
    "    print(f\"ðŸš€ Step 1: Caching probability maps for ALL {len(val_set)} images...\")\n",
    "    cache = []\n",
    "    for p, label in tqdm(val_set):\n",
    "        pil = Image.open(p).convert(\"RGB\")\n",
    "        w, h = pil.size\n",
    "\n",
    "        prob = segment_prob_map_with_tta(pil) if USE_TTA else segment_prob_map(pil)\n",
    "\n",
    "        mask_resized, _ = finalize_mask(prob, (w, h))\n",
    "\n",
    "        if label == \"forged\":\n",
    "            m_gt = np.load(Path(mask_dir)/f\"{Path(p).stem}.npy\")\n",
    "            if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
    "            m_gt = (m_gt > 0).astype(np.uint8)\n",
    "        else:\n",
    "            m_gt = np.zeros((h, w), np.uint8)\n",
    "\n",
    "        cache.append({\"prob\": prob, \"mask\": mask_resized, \"gt\": m_gt, \"label\": label})\n",
    "\n",
    "    best_f1 = -1\n",
    "    best_params = {}\n",
    "    combinations = list(itertools.product(area_range, mean_range))\n",
    "\n",
    "    for a_thr, m_thr in combinations:\n",
    "        current_f1s = []\n",
    "        for item in cache:\n",
    "            mask = item[\"mask\"]\n",
    "            area = int(mask.sum())\n",
    "\n",
    "            mask_small = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "            mean_in = float(item[\"prob\"][mask_small == 1].mean()) if area > 0 else 0.0\n",
    "\n",
    "            is_forged = (area >= a_thr and mean_in >= m_thr)\n",
    "            m_pred = (mask > 0).astype(np.uint8) if is_forged else np.zeros_like(item[\"gt\"])\n",
    "\n",
    "            f1 = f1_score(item[\"gt\"].flatten(), m_pred.flatten(),\n",
    "                          zero_division=1 if item[\"label\"] == \"authentic\" else 0)\n",
    "            current_f1s.append(f1)\n",
    "\n",
    "        avg_f1 = np.mean(current_f1s)\n",
    "        if avg_f1 > best_f1:\n",
    "            best_f1 = avg_f1\n",
    "            best_params = {\"AREA_THR\": a_thr, \"MEAN_THR\": m_thr}\n",
    "            print(f\"â­ New Best F1: {best_f1:.4f} -> AREA: {a_thr}, MEAN: {m_thr}\")\n",
    "\n",
    "    return best_params\n",
    "\n",
    "if GRID_SEARCH:\n",
    "    best_cfg = grid_search_area_mean(val_forg, val_auth, MASK_DIR)\n",
    "    AREA_THR = best_cfg['AREA_THR']\n",
    "    MEAN_THR = best_cfg['MEAN_THR']\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "val_items = [(p, 1) for p in val_forg[:10]]\n",
    "results = []\n",
    "for p,_ in tqdm(val_items, desc=\"Validation forged-only\"):\n",
    "    pil = Image.open(p).convert(\"RGB\")\n",
    "    label, m_pred, dbg = pipeline_final(pil)\n",
    "    m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
    "    if m_gt.ndim==3: m_gt=np.max(m_gt,axis=0)\n",
    "    m_gt=(m_gt>0).astype(np.uint8)\n",
    "    m_pred=(m_pred>0).astype(np.uint8) if m_pred is not None else np.zeros_like(m_gt)\n",
    "    f1 = f1_score(m_gt.flatten(), m_pred.flatten(), zero_division=0)\n",
    "    results.append((Path(p).stem, f1, dbg))\n",
    "print(\"\\n F1-score par image falsifiÃ©e:\\n\")\n",
    "for cid,f1,dbg in results:\n",
    "    print(f\"{cid} â€” F1={f1:.4f} | area={dbg['area']} mean={dbg['mean_inside']:.3f} thr={dbg['thr']:.3f}\")\n",
    "print(f\"\\n Moyenne F1 (falsifiÃ©es) = {np.mean([r[1] for r in results]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf6fc9",
   "metadata": {
    "id": "f8bf6fc9",
    "papermill": {
     "duration": 0.003742,
     "end_time": "2026-01-05T16:09:17.618702",
     "exception": false,
     "start_time": "2026-01-05T16:09:17.614960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 2: Hybrid Model â€” DINOv2 Feature Extraction & CNN Decoder Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QRFLnT1PDQKe",
   "metadata": {
    "id": "QRFLnT1PDQKe"
   },
   "outputs": [],
   "source": [
    "GRID_SEARCH = True\n",
    "if GRID_SEARCH:\n",
    "    best_cfg = grid_search_area_mean(val_forg, val_auth, MASK_DIR)\n",
    "    AREA_THR = best_cfg['AREA_THR']\n",
    "    MEAN_THR = best_cfg['MEAN_THR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pVJDaq_vClSC",
   "metadata": {
    "id": "pVJDaq_vClSC"
   },
   "outputs": [],
   "source": [
    "USE_TTA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1da3ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T16:09:17.631211Z",
     "iopub.status.busy": "2026-01-05T16:09:17.630298Z",
     "iopub.status.idle": "2026-01-05T16:09:19.597203Z",
     "shell.execute_reply": "2026-01-05T16:09:19.596114Z"
    },
    "id": "0a1da3ff",
    "papermill": {
     "duration": 1.97579,
     "end_time": "2026-01-05T16:09:19.599643",
     "exception": false,
     "start_time": "2026-01-05T16:09:17.623853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, json, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- RLE Encoder for Kaggle Submission ---\n",
    "def rle_encode(mask: np.ndarray, fg_val: int = 1) -> str:\n",
    "    pixels = mask.T.flatten()\n",
    "    dots = np.where(pixels == fg_val)[0]\n",
    "    if len(dots) == 0:\n",
    "        return \"authentic\"\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if b > prev + 1:\n",
    "            run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return json.dumps([int(x) for x in run_lengths])\n",
    "\n",
    "# --- Paths (dynamic with BASE_DIR from earlier cell) ---\n",
    "TEST_DIR = f\"{BASE_DIR}/test_images\"\n",
    "SAMPLE_SUB = f\"{BASE_DIR}/sample_submission.csv\"\n",
    "OUT_PATH = \"submission.csv\"\n",
    "\n",
    "rows = []\n",
    "for f in tqdm(sorted(os.listdir(TEST_DIR)), desc=\"Inference on Test Set\"):\n",
    "    pil = Image.open(Path(TEST_DIR)/f).convert(\"RGB\")\n",
    "    label, mask, dbg = pipeline_final(pil)  # utilise la version amÃ©liorÃ©e\n",
    "\n",
    "    # SÃ©curisation masque\n",
    "    if mask is None:\n",
    "        mask = np.zeros(pil.size[::-1], np.uint8)\n",
    "    else:\n",
    "        mask = np.array(mask, dtype=np.uint8)\n",
    "\n",
    "    # Annotation finale\n",
    "    if label == \"authentic\":\n",
    "        annot = \"authentic\"\n",
    "    else:\n",
    "        annot = rle_encode((mask > 0).astype(np.uint8))\n",
    "\n",
    "    rows.append({\n",
    "        \"case_id\": Path(f).stem,\n",
    "        \"annotation\": annot,\n",
    "        \"area\": int(dbg.get(\"area\", mask.sum())),\n",
    "        \"mean\": float(dbg.get(\"mean_inside\", 0.0)),\n",
    "        \"thr\": float(dbg.get(\"thr\", 0.0))\n",
    "    })\n",
    "\n",
    "\n",
    "sub = pd.DataFrame(rows)\n",
    "ss = pd.read_csv(SAMPLE_SUB)\n",
    "ss[\"case_id\"] = ss[\"case_id\"].astype(str)\n",
    "sub[\"case_id\"] = sub[\"case_id\"].astype(str)\n",
    "final = ss[[\"case_id\"]].merge(sub, on=\"case_id\", how=\"left\")\n",
    "final[\"annotation\"] = final[\"annotation\"].fillna(\"authentic\")\n",
    "final[[\"case_id\", \"annotation\"]].to_csv(OUT_PATH, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Saved submission file: {OUT_PATH}\")\n",
    "print(final.head(10))\n",
    "\n",
    "\n",
    "sample_files = sorted(os.listdir(TEST_DIR))[:5]\n",
    "for f in sample_files:\n",
    "    pil = Image.open(Path(TEST_DIR)/f).convert(\"RGB\")\n",
    "    label, mask, dbg = pipeline_final(pil)\n",
    "    mask = np.array(mask, dtype=np.uint8) if mask is not None else np.zeros(pil.size[::-1], np.uint8)\n",
    "\n",
    "    print(f\"{'ðŸ”´' if label=='forged' else 'ðŸŸ¢'} {f}: {label} | area={mask.sum()} mean={dbg.get('mean_inside', 0):.3f}\")\n",
    "\n",
    "    if label == \"authentic\":\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(pil)\n",
    "        plt.title(f\"{f} â€” Authentic\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(pil)\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(pil)\n",
    "        plt.imshow(mask, alpha=0.45, cmap=\"Reds\")\n",
    "        plt.title(f\"Predicted Forged Mask\\nArea={mask.sum()} | Mean={dbg.get('mean_inside', 0):.3f}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ef216",
   "metadata": {
    "id": "b40ef216",
    "papermill": {
     "duration": 0.004832,
     "end_time": "2026-01-05T16:09:19.609605",
     "exception": false,
     "start_time": "2026-01-05T16:09:19.604773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ðŸ”´ Visualizing Predicted Masks with the CNNâ€“DINOv2 Hybrid Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535b22f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T16:09:19.621428Z",
     "iopub.status.busy": "2026-01-05T16:09:19.620967Z",
     "iopub.status.idle": "2026-01-05T16:09:25.483926Z",
     "shell.execute_reply": "2026-01-05T16:09:25.482899Z"
    },
    "id": "6535b22f",
    "papermill": {
     "duration": 5.880941,
     "end_time": "2026-01-05T16:09:25.495264",
     "exception": false,
     "start_time": "2026-01-05T16:09:19.614323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch, cv2, math, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Use global IMG_SIZE from cell 2 (518)\n",
    "\n",
    "# 1ï¸ Predict probability map (from model)\n",
    "@torch.no_grad()\n",
    "def predict_prob_map(pil):\n",
    "    \"\"\"Return DINOv2 segmentation probability map [0,1].\"\"\"\n",
    "    img = pil.resize((IMG_SIZE, IMG_SIZE))\n",
    "    x = torch.from_numpy(np.array(img, np.float32) / 255.).permute(2, 0, 1)[None].to(device)\n",
    "    logits = model_seg.forward_seg(x)\n",
    "    prob = torch.sigmoid(logits)[0, 0].cpu().numpy()\n",
    "    return prob\n",
    "\n",
    "\n",
    "# 2ï¸ Post-processing consistent with pipeline_final\n",
    "def adaptive_mask(prob, alpha_grad=0.35):\n",
    "    \"\"\"Adaptive enhancement + morphological refinement.\"\"\"\n",
    "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
    "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
    "\n",
    "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
    "    enhanced = cv2.GaussianBlur(enhanced, (3, 3), 0)\n",
    "\n",
    "    thr = np.mean(enhanced) + 0.3 * np.std(enhanced)\n",
    "    mask = (enhanced > thr).astype(np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8))\n",
    "    return mask, float(thr)\n",
    "\n",
    "\n",
    "# 3ï¸ Unified visualization pipeline (uses same filtering logic as pipeline_final)\n",
    "def pipeline_visual(pil):\n",
    "    if USE_TTA:\n",
    "        prob = segment_prob_map_with_tta(pil)\n",
    "    else:\n",
    "        prob = predict_prob_map(pil)\n",
    "    mask, thr = adaptive_mask(prob)\n",
    "    area = int(mask.sum())\n",
    "    mean_inside = float(prob[mask == 1].mean()) if area > 0 else 0.0\n",
    "\n",
    "    # âœ… FIXED: Use same decision rule as pipeline_final for consistency\n",
    "    if area < AREA_THR or mean_inside < MEAN_THR:\n",
    "        label = \"authentic\"\n",
    "    else:\n",
    "        label = \"forged\"\n",
    "    return label, mask, thr, area, mean_inside\n",
    "\n",
    "\n",
    "# 4ï¸ Visualization (for validation forged samples)\n",
    "sample_forged = val_forg[:5]\n",
    "n = len(sample_forged)\n",
    "fig, axes = plt.subplots(n, 3, figsize=(12, n * 3))\n",
    "if n == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "for i, p in enumerate(sample_forged):\n",
    "    pil = Image.open(p).convert(\"RGB\")\n",
    "    label, m_pred, thr, area, mean = pipeline_visual(pil)\n",
    "\n",
    "    # Ground Truth mask\n",
    "    m_gt = np.load(Path(MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
    "    if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
    "    m_gt = (m_gt > 0).astype(np.uint8)\n",
    "\n",
    "    # Resize all for consistency\n",
    "    img_disp = cv2.resize(np.array(pil), (IMG_SIZE, IMG_SIZE))\n",
    "    gt_disp  = cv2.resize(m_gt, (IMG_SIZE, IMG_SIZE))\n",
    "    pr_disp  = cv2.resize(m_pred, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # === Column 1: Original ===\n",
    "    axes[i, 0].imshow(img_disp)\n",
    "    axes[i, 0].set_title(\"ðŸ–¼ï¸ Original Image\", fontsize=11, weight=\"bold\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    # === Column 2: Ground Truth ===\n",
    "    axes[i, 1].imshow(gt_disp, cmap=\"gray\")\n",
    "    axes[i, 1].set_title(\"âœ… Ground Truth\", fontsize=11, weight=\"bold\")\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "    # === Column 3: Predicted Mask ===\n",
    "    axes[i, 2].imshow(img_disp)\n",
    "    axes[i, 2].imshow(pr_disp, cmap=\"coolwarm\", alpha=0.45)\n",
    "    axes[i, 2].set_title(f\"ðŸ”® Predicted ({label})\\nThr={thr:.3f} | Area={area} | Mean={mean:.3f}\",\n",
    "                         fontsize=10)\n",
    "    axes[i, 2].axis(\"off\")\n",
    "\n",
    "plt.subplots_adjust(top=0.92, hspace=0.35)\n",
    "fig.suptitle(\"ðŸ” Segmentation of Forged Samples â€” CNNâ€“DINOv2 Hybrid\",\n",
    "             fontsize=16, fontweight=\"bold\", color=\"#b30000\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546963b",
   "metadata": {
    "id": "e546963b",
    "papermill": {
     "duration": 0.014581,
     "end_time": "2026-01-05T16:09:25.524396",
     "exception": false,
     "start_time": "2026-01-05T16:09:25.509815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ðŸŸ¢ Visualization of Authentic Images (Hybrid DINOv2-based Detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cab85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-05T16:09:25.555539Z",
     "iopub.status.busy": "2026-01-05T16:09:25.555093Z",
     "iopub.status.idle": "2026-01-05T16:09:30.278593Z",
     "shell.execute_reply": "2026-01-05T16:09:30.277528Z"
    },
    "id": "7e0cab85",
    "papermill": {
     "duration": 4.748098,
     "end_time": "2026-01-05T16:09:30.287064",
     "exception": false,
     "start_time": "2026-01-05T16:09:25.538966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2, numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Select a few authentic examples\n",
    "sample_auth = val_auth[:5]\n",
    "n = len(sample_auth)\n",
    "\n",
    "fig, axes = plt.subplots(n, 2, figsize=(9, n * 3))\n",
    "if n == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "for i, p in enumerate(sample_auth):\n",
    "    pil = Image.open(p).convert(\"RGB\")\n",
    "    label, m_pred, thr, area, mean = pipeline_visual(pil)  # <-- version alignÃ©e avec ta nouvelle pipeline\n",
    "\n",
    "    # Predicted mask (should be empty for authentic images)\n",
    "    m_pred = (m_pred > 0).astype(np.uint8) if m_pred is not None else np.zeros((IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # Resize for consistent display\n",
    "    img_disp = cv2.resize(np.array(pil), (IMG_SIZE, IMG_SIZE))\n",
    "    pr_disp  = cv2.resize(m_pred, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # === Column 1: Original Image ===\n",
    "    axes[i, 0].imshow(img_disp)\n",
    "    axes[i, 0].set_title(\"ðŸ–¼ï¸ Original Image\", fontsize=11, weight=\"bold\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    # === Column 2: Predicted Mask ===\n",
    "    axes[i, 1].imshow(img_disp)\n",
    "    axes[i, 1].imshow(pr_disp, cmap=\"coolwarm\", alpha=0.45)\n",
    "    axes[i, 1].set_title(\n",
    "        f\"ðŸŸ¢ Predicted: {label.upper()}\\nArea={area} | Mean={mean:.3f} | Thr={thr:.3f}\",\n",
    "        fontsize=10\n",
    "    )\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "    for j in range(2):\n",
    "        axes[i, j].set_aspect(\"equal\")\n",
    "\n",
    "plt.subplots_adjust(top=0.90, hspace=0.35)\n",
    "fig.suptitle(\"ðŸŸ¢ Segmentation of Authentic Images â€” CNNâ€“DINOv2 Hybrid\",\n",
    "             fontsize=16, fontweight=\"bold\", color=\"#009933\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XgpLNajPG1Jg",
   "metadata": {
    "id": "XgpLNajPG1Jg"
   },
   "source": [
    "## official validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18yNx9pgGyzF",
   "metadata": {
    "id": "18yNx9pgGyzF"
   },
   "outputs": [],
   "source": [
    "def gt_masks_from_npy(mask_path: Path) -> list[np.ndarray]:\n",
    "\n",
    "    m = np.load(mask_path)\n",
    "\n",
    "    if m.ndim == 3:\n",
    "\n",
    "        parts = [(m[k] > 0).astype(np.uint8) for k in range(m.shape[0])]\n",
    "\n",
    "        return parts\n",
    "\n",
    "    else:\n",
    "\n",
    "        return [((m > 0).astype(np.uint8))]\n",
    "\n",
    "# --- Validation using official oF1 on forged validation set ---\n",
    "print(\"\\n--- Official oF1 Validation ---\")\n",
    "\n",
    "SUPPLEMENT_DIR  = f\"{BASE_DIR}/supplemental_images\"\n",
    "SUPPLE_MASK_DIR  = f\"{BASE_DIR}/supplemental_masks\"\n",
    "supplemental_imgs = sorted([str(Path(SUPPLEMENT_DIR)/f) for f in os.listdir(SUPPLEMENT_DIR)])\n",
    "supplemental_mask_imgs = sorted([str(Path(SUPPLE_MASK_DIR)/f) for f in os.listdir(SUPPLE_MASK_DIR)])\n",
    "\n",
    "val_items = supplemental_imgs\n",
    "\n",
    "scores = []\n",
    "\n",
    "for p in tqdm(val_items, desc=\"oF1 forged-only\"):\n",
    "\n",
    "    pil = Image.open(p).convert(\"RGB\")\n",
    "    label, m_pred, dbg = pipeline_final(pil)\n",
    "\n",
    "    # Ground-truth masks (instance-aware)\n",
    "    gt_list = gt_masks_from_npy(Path(SUPPLE_MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
    "\n",
    "    # Predicted masks list: empty if classified authentic; else single mask\n",
    "    if label == \"authentic\" or m_pred is None:\n",
    "        pred_list = []\n",
    "    else:\n",
    "        pred = (np.array(m_pred) > 0).astype(np.uint8)\n",
    "        pred_list = [pred]\n",
    "\n",
    "    # Compute official oF1\n",
    "    scores.append(oF1_score(pred_list, gt_list))\n",
    "\n",
    "print(f\"Average oF1 (Forged Validation) = {np.mean(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JI0esF7fLGw6",
   "metadata": {
    "id": "JI0esF7fLGw6"
   },
   "outputs": [],
   "source": [
    "# 4ï¸ Visualization (for validation forged samples)\n",
    "sample_forged = val_items[:5]\n",
    "n = len(sample_forged)\n",
    "fig, axes = plt.subplots(n, 3, figsize=(12, n * 3))\n",
    "if n == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "for i, p in enumerate(sample_forged):\n",
    "    pil = Image.open(p).convert(\"RGB\")\n",
    "    label, m_pred, thr, area, mean = pipeline_visual(pil)\n",
    "\n",
    "    # Ground Truth mask\n",
    "    m_gt = np.load(Path(SUPPLE_MASK_DIR)/f\"{Path(p).stem}.npy\")\n",
    "    if m_gt.ndim == 3: m_gt = np.max(m_gt, axis=0)\n",
    "    m_gt = (m_gt > 0).astype(np.uint8)\n",
    "\n",
    "    # Resize all for consistency\n",
    "    img_disp = cv2.resize(np.array(pil), (IMG_SIZE, IMG_SIZE))\n",
    "    gt_disp  = cv2.resize(m_gt, (IMG_SIZE, IMG_SIZE))\n",
    "    pr_disp  = cv2.resize(m_pred, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # === Column 1: Original ===\n",
    "    axes[i, 0].imshow(img_disp)\n",
    "    axes[i, 0].set_title(\"ðŸ–¼ï¸ Original Image\", fontsize=11, weight=\"bold\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    # === Column 2: Ground Truth ===\n",
    "    axes[i, 1].imshow(gt_disp, cmap=\"gray\")\n",
    "    axes[i, 1].set_title(\"âœ… Ground Truth\", fontsize=11, weight=\"bold\")\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "    # === Column 3: Predicted Mask ===\n",
    "    axes[i, 2].imshow(img_disp)\n",
    "    axes[i, 2].imshow(pr_disp, cmap=\"coolwarm\", alpha=0.45)\n",
    "    axes[i, 2].set_title(f\"ðŸ”® Predicted ({label})\\nThr={thr:.3f} | Area={area} | Mean={mean:.3f}\",\n",
    "                         fontsize=10)\n",
    "    axes[i, 2].axis(\"off\")\n",
    "\n",
    "plt.subplots_adjust(top=0.92, hspace=0.35)\n",
    "fig.suptitle(\"ðŸ” Segmentation of Forged Samples â€” CNNâ€“DINOv2 Hybrid\",\n",
    "             fontsize=16, fontweight=\"bold\", color=\"#b30000\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CBlPkKss8CBw",
   "metadata": {
    "id": "CBlPkKss8CBw"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b52bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Unify adaptive mask alpha (single source of truth) ---\n",
    "import numpy as np, cv2\n",
    "\n",
    "# Unified function: use alpha_grad=0.45 everywhere\n",
    "\n",
    "def adaptive_mask(prob: np.ndarray, alpha_grad: float = 0.45):\n",
    "    gx = cv2.Sobel(prob, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    gy = cv2.Sobel(prob, cv2.CV_32F, 0, 1, ksize=3)\n",
    "    grad_mag = np.sqrt(gx**2 + gy**2)\n",
    "    grad_norm = grad_mag / (grad_mag.max() + 1e-6)\n",
    "    enhanced = (1 - alpha_grad) * prob + alpha_grad * grad_norm\n",
    "    enhanced = cv2.GaussianBlur(enhanced, (3, 3), 0)\n",
    "    thr = float(np.mean(enhanced) + 0.3 * np.std(enhanced))\n",
    "    mask = (enhanced > thr).astype(np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8))\n",
    "    return mask, thr\n",
    "\n",
    "# Backward-compat: make any previous names point to the unified one\n",
    "\n",
    "def enhanced_adaptive_mask(prob: np.ndarray):\n",
    "    return adaptive_mask(prob, alpha_grad=0.45)\n",
    "\n",
    "# Redefine finalize_mask to use unified adaptive_mask\n",
    "\n",
    "def finalize_mask(prob: np.ndarray, orig_size: tuple[int, int]):\n",
    "    mask, thr = adaptive_mask(prob, alpha_grad=0.45)\n",
    "    mask = cv2.resize(mask, orig_size, interpolation=cv2.INTER_NEAREST)\n",
    "    return mask, thr\n",
    "\n",
    "print(\"âœ… Adaptive mask alpha unified to 0.45 for evaluation/inference.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14878066,
     "sourceId": 113558,
     "sourceType": "competition"
    },
    {
     "datasetId": 9153851,
     "sourceId": 14354477,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 986,
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 113.470973,
   "end_time": "2026-01-05T16:09:32.986883",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-05T16:07:39.515910",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
